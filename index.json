[{"content":"A Server Monitoring sorozat folytatásaként most a docker container logok gyűjtésével és megjelenítésével foglalkozom.\nDocker container logok gyűjtése A container logok helye: /var/lib/docker/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log\nAhol a \u0026lt;container-id\u0026gt; a docker container-ek egyedi azonosítója.\nA docker container logok gyűjtéséhez a Promtail service számára a /var/lib/docker könyvtárat kell volume-ként becsatolni:\npromtail: image: grafana/promtail:latest container_name: promtail command: -config.file=/etc/promtail/config.yml volumes: - ./promtail/promtail-config.yml:/etc/promtail/config.yml - /var/log:/var/log:ro - /var/lib/docker/:/var/lib/docker:ro restart: unless-stopped Így a promtail container eléri a host docker log-jait és most már be tudjuk állítani, hogy a promtail a /var/lib/docker/containers/*/*log fájlokat figyelve a logokat elküldje a Loki-nak. Ehhez a promtail/promtail-config.yml fájlba a scrape_configs alá egy új job-ot kell felvenni:\n- job_name: containers static_configs: - targets: - localhost labels: job: containerlogs host: raspberrypi __path__: /var/lib/docker/containers/*/*log Docker log kimenet kiegészítése Ha csak a fenti beállításokat használnánk, akkor Grafana-ban csak a \u0026lt;container-id\u0026gt;-json.log fájlnevek állnának rendelkezésünkre, amik nem teszik egyszerűvé egy kívánt container log-jának megtekintését, mert ahhoz kellene tudnunk, hogy a vizsgálni kívánt container-nek mi az azonosítója.\nA json-file log driver esetében van lehetőségünk módosítani a log kimenetet. A log-opts paraméterben adható meg a log tag-je, ami azonosítja a container log üzenetét. Alapesetben a container id-ja lesz felhasználva, amit felülbírálhatunk.\nA tag értékének az alábbit választottam:\n{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}} Azaz tartalmazza a docker image nevét, a container nevét, a docker image teljes id-ját és a container teljes id-ját.\nKét lehetőségünk van a tag beállítására: adott container indításánál vagy globálisan, így minden újjonnan létrehozott container esetében érvényesül az érték.\nGlobálisan a /etc/docker/daemon.json fájlban adható meg:\n{ \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}\u0026#34; } } A beállítás érvényesítéséhez újra kell indítani a docker-t:\nsudo systemctl restart docker Illetve a módosítás csak új container-ek esetében kerül alkalmazásra.\nPromtail pipeline Most, hogy a container logok kiegészültek a container-ek nevével is, promtail-ben ezen extra adatokat ki kell nyernünk, hogy a Loki felé mint speciális mezők küldje el. Erre használható a Promtail Pipeline.\nEnnek felhasználásával a promtail/promtail-config.yml fájlban felvett containers job kiegészítve egy pipeline-al, ami kiveszi a container logokból az image nevet, a container nevet, az image id-t és a container id-t, az alábbi módon néz ki:\n- job_name: system static_configs: - targets: - localhost labels: job: varlogs host: raspberrypi __path__: /var/log/*log - job_name: containers static_configs: - targets: - localhost labels: job: containerlogs host: raspberrypi __path__: /var/lib/docker/containers/*/*log # --log-opt tag=\u0026#34;{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}\u0026#34; pipeline_stages: - json: expressions: stream: stream attrs: attrs tag: attrs.tag - regex: expression: (?P\u0026lt;image_name\u0026gt;(?:[^|]*[^|])).(?P\u0026lt;container_name\u0026gt;(?:[^|]*[^|])).(?P\u0026lt;image_id\u0026gt;(?:[^|]*[^|])).(?P\u0026lt;container_id\u0026gt;(?:[^|]*[^|])) source: tag - labels: tag: stream: image_name: container_name: image_id: container_id A beállítások érvényesítéséhez a futó promtail container-t le kell állítanunk, majd törölnünk és újra létrehoznunk.\nContainer logok megjelenítése Most, hogy az új container log kimenetet megadtuk, promtail-t beállítottuk, hogy a log sorokból kinyerje az adott container nevét, Grafana-ban lehetőségünk van egy-egy container logjának megtekintésére.\nAz Explore menüben Loki-t mint datasource-t kiválasztva a Log browser-en belül a label-ek közt megtaláljuk az alábbiakat is:\n container_id container_name image_id image_name  Ezen új label-ek azok, amelyeket promtail tett elérhetővé.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-09-server-monitoring-part-05-container-logs/","summary":"A Server Monitoring sorozat folytatásaként most a docker container logok gyűjtésével és megjelenítésével foglalkozom.\nDocker container logok gyűjtése A container logok helye: /var/lib/docker/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log\nAhol a \u0026lt;container-id\u0026gt; a docker container-ek egyedi azonosítója.\nA docker container logok gyűjtéséhez a Promtail service számára a /var/lib/docker könyvtárat kell volume-ként becsatolni:\npromtail: image: grafana/promtail:latest container_name: promtail command: -config.file=/etc/promtail/config.yml volumes: - ./promtail/promtail-config.yml:/etc/promtail/config.yml - /var/log:/var/log:ro - /var/lib/docker/:/var/lib/docker:ro restart: unless-stopped Így a promtail container eléri a host docker log-jait és most már be tudjuk állítani, hogy a promtail a /var/lib/docker/containers/*/*log fájlokat figyelve a logokat elküldje a Loki-nak.","title":"Raspberry Pi 3 - Part 9: Server Monitoring Part 5 - Container Logs"},{"content":"A Server Monitoring sorozat folytatásaként most a linux host system log gyűjtésével és megjelenítésével foglalkozom.\nSystem log gyűjtése A system log gyűjtéséhez a Promtail service számára a linux host /var/log könyvtárát kell volume-ként becsatolni. Az eredeti docker-compose promtail service ezzel kiegészítve az alábbi:\npromtail: image: grafana/promtail:latest container_name: promtail command: -config.file=/etc/promtail/config.yml volumes: - ./promtail/promtail-config.yml:/etc/promtail/config.yml - /var/log:/var/log:ro restart: unless-stopped Így a promtail container eléri a host system log-jait és most már be tudjuk állítani, hogy a promtail a /var/log könyvtárat figyelve a logokat elküldje a Loki-nak. Ehhez a promtail/promtail-config.yml fájlba a scrape_configs alá egy új job-ot kell felvennünk:\n- job_name: system static_configs: - targets: - localhost labels: job: varlogs host: raspberrypi __path__: /var/log/*log A beállítások érvényesítéséhez a futó promtail container-t le kell állítanunk, majd törölnünk és újra létrehoznunk.\nSystem log megjelenítése Grafana-ban az Explore menüben Loki-t mint datasource-t kiválasztva van lehetőségünk a logok megtekintésére. Itt forrásként megadhatjuk a varlogs nevű job-ot, vagy a raspberrypi nevű host-ot és így egyszerre megtekinthető az összes log, vagy lehetőségünk van egy logot a fájl név alapján megtekinteni.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-08-server-monitoring-part-04-system-logs/","summary":"A Server Monitoring sorozat folytatásaként most a linux host system log gyűjtésével és megjelenítésével foglalkozom.\nSystem log gyűjtése A system log gyűjtéséhez a Promtail service számára a linux host /var/log könyvtárát kell volume-ként becsatolni. Az eredeti docker-compose promtail service ezzel kiegészítve az alábbi:\npromtail: image: grafana/promtail:latest container_name: promtail command: -config.file=/etc/promtail/config.yml volumes: - ./promtail/promtail-config.yml:/etc/promtail/config.yml - /var/log:/var/log:ro restart: unless-stopped Így a promtail container eléri a host system log-jait és most már be tudjuk állítani, hogy a promtail a /var/log könyvtárat figyelve a logokat elküldje a Loki-nak.","title":"Raspberry Pi 3 - Part 8: Server Monitoring Part 4 - System Logs"},{"content":"A Server Monitoring sorozat folytatásaként most a futó docker containerek erőforrás igényeit szeretném kinyerni és megjeleníteni.\nDocker daemon Van lehetőség közvetlenül a docker daemon-ból kinyerni metric adatokat, a hivatalos docker dokumentációban találtam leírást hozzá. Jelenleg még fejlesztés alatt van, így csak experimental módban lehet használni.\nA /etc/docker/daemon.json fájlban adható meg a metric végpont engedélyezése:\n{ \u0026#34;metrics-addr\u0026#34; : \u0026#34;127.0.0.1:9323\u0026#34;, \u0026#34;experimental\u0026#34; : true } Ahhoz, hogy érvényesüljön a beállítás újra kell indítani a docker daemon-t:\nsudo systemctl restart docker.service Ezt követően a docker daemon a 9323-as porton Prometheus kompatibilis metric adatokat szolgáltat, ezt a következőképp ellenőrizhetjük:\nPort ellenőrzése:\nsudo lsof -i -P -n | grep LISTEN | grep 9323 Metric végpont ellenőrzése:\ncurl localhost:9323/metrics Előre elkészített Grafana dashboard-ot nem találtam a docker daemon-hoz, illetve a docker daemon által szolgáltatott metric végponton a docker daemon internal állapotáról kapunk metric adatokat és a futó docker container-ekről, azok erőforrás igényéről nem. Így ezt nem tudom felhasználni.\ncAdvisor A cAdvisor egy igen népszerű eszköz, azonban sajnos arm architektúrára nincs docker image-ük, de vannak nem hivatalos arm buildek, ezek közül a zcube/cadvisor-t választottam.\nA cAdvisor docker-compose service-ként az alábbi módon hoztam létre:\ncadvisor: image: zcube/cadvisor:latest container_name: cadvisor ports: - \u0026#34;8080:8080\u0026#34; volumes: - /:/rootfs:ro - /var/run:/var/run:ro - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro - /dev/disk/:/dev/disk:ro devices: - /dev/kmsg privileged: true ipc: shareable security_opt: - label=disable restart: unless-stopped amely a grafana-stack docker-compose.yml fájlba vettem fel, hogy egy docker network-ön legyen a stack többi részével.\ncAdvisor metric adatok gyűjtése Ahhoz, hogy Prometheus elérje a cAdvicser metric végpontját azt a prometheus.yml fájlban fel kell venni a scrape_configs rész alá:\nglobal: scrape_interval: 5s scrape_configs: - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] A módosítást, kiegészítést követően újra kell olvastatni Prometheus-al a config fájlt.\ncAdvisor metric adatok megjelenítése A cAdvisor által szolgáltatott metric adatok, most már Prometheus-ba kerülnek, így egy grafana dashboard-ot is választottam: Docker and OS Metrics for Raspberry Pi by oijkn\nEz egyben a node exporter adatokból is dolgozik, nemcsak a cAdvisor adataiból, így ezt igen praktikusnak találom.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-07-server-monitoring-part-03-container-metrics/","summary":"A Server Monitoring sorozat folytatásaként most a futó docker containerek erőforrás igényeit szeretném kinyerni és megjeleníteni.\nDocker daemon Van lehetőség közvetlenül a docker daemon-ból kinyerni metric adatokat, a hivatalos docker dokumentációban találtam leírást hozzá. Jelenleg még fejlesztés alatt van, így csak experimental módban lehet használni.\nA /etc/docker/daemon.json fájlban adható meg a metric végpont engedélyezése:\n{ \u0026#34;metrics-addr\u0026#34; : \u0026#34;127.0.0.1:9323\u0026#34;, \u0026#34;experimental\u0026#34; : true } Ahhoz, hogy érvényesüljön a beállítás újra kell indítani a docker daemon-t:","title":"Raspberry Pi 3 - Part 7: Server Monitoring Part 3 - Container Metrics"},{"content":"A Server Monitoring sorozat folytatásaként most a hardware és os metric adatok gyűjtésével és megjelenítésével foglalkozom.\nSystem metric adatok elérése A Prometheus egyik hivatalos metric exporter-je a Node exporter , amely pont megfelel céljaim számára.\nA node exporter futtatható docker container-ként és docker-compose service-ként az alábbi módon hoztam létre:\nnode_exporter: image: quay.io/prometheus/node-exporter:latest container_name: node_exporter command: --path.rootfs=/host pid: host ports: - \u0026#34;9100:9100\u0026#34; volumes: - /:/host:ro,rslave restart: unless-stopped amelyet a grafana-stack docker-compose.yml fájlba vettem fel, hogy egy docker network-on legyen a stack többi részével.\nSystem metric adatok gyűjtése Annak érdekében, hogy Prometheus tudjon a node exporter által publikált /metrics végponton szolgáltatott metric adatokat gyűjteni, úgy azt fel kell venni a prometheus.yml fájlban a scrape_configs-ok alá:\nglobal: scrape_interval: 5s scrape_configs: - job_name: \u0026#39;node_exporter\u0026#39; static_configs: - targets: [\u0026#39;node_exporter:9100\u0026#39;] Annak érdekben, hogy érvényesüljenek a módosításaink Prometheus-nak újra kell olvasnia a config fájlt. Ez elérhető a container újraindításával, vagy ha aktiváltuk a futás idejű config fájl újraolvasását, úgy a serveren futtassuk az alábbi parancsot:\ncurl -X POST http://localhost:9090/-/reload Ezt követően a Prometheus web ui-ban a Targets alatt látnunk kell az imént felvett node_exporter nevű job-ot.\nSystem metric adatok megjelenítése Miután a node exporter által szolgáltatott metric adatok Prometheus-ban tárolásra kerülnek, úgy a Grafana-ban lehetőségünk van ezen metric adatokat megjeleníteni.\nNagyon sok előre elkészített dashboard létezik a node exporter-hez, az én választásom a Node Exporter Full by rfraile-ra esett.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-06-server-monitoring-part-02-system-metrics/","summary":"A Server Monitoring sorozat folytatásaként most a hardware és os metric adatok gyűjtésével és megjelenítésével foglalkozom.\nSystem metric adatok elérése A Prometheus egyik hivatalos metric exporter-je a Node exporter , amely pont megfelel céljaim számára.\nA node exporter futtatható docker container-ként és docker-compose service-ként az alábbi módon hoztam létre:\nnode_exporter: image: quay.io/prometheus/node-exporter:latest container_name: node_exporter command: --path.rootfs=/host pid: host ports: - \u0026#34;9100:9100\u0026#34; volumes: - /:/host:ro,rslave restart: unless-stopped amelyet a grafana-stack docker-compose.yml fájlba vettem fel, hogy egy docker network-on legyen a stack többi részével.","title":"Raspberry Pi 3 - Part 6: Server Monitoring Part 2 - System Metrics"},{"content":"A Raspberry Pi server monitorozását a Grafana Labs eszközeivel kívánom megoldani.\nA célom a rendszer és a docker container-ek monitorozása, az adatok megjelenítése grafikonokon - úgynevezett dashboard-on -, a rendszer és docker container logok egységes kezelése, riasztások küldése nem kívánt állapotok esetén.\nA Grafana Labs főbb eszközei:\n Grafana: metric adat vizualizáló, kereső és riszasztás kezelő rendszer Prometheus: metric adatokat gyűjtő és tároló rendszer Loki: log aggregáló rendszer Promtail: agent, amely logokat továbbít Loki-nak.  Grafana stack létrehozása A Grafana stack elemeit docker container-ekben kívánom futtatni.\nEnnek érdekében elsőként hozzunk létre egy könytárat, ahol tárolni fogjuk a stack config adatait:\nmkdir /opt/services/grafana-stack Amely tartalmazza az alábbi könyvtárakat és fájlokat:\n. ├── docker-compose.yml ├── grafana │ └── provisioning │ └── datasources │ ├── loki_ds.yml │ └── prometheus_ds.yml ├── prometheus │ └── prometheus.yml └── promtail └── promtail-config.yml Grafana stack docker-compose.yml A grafana stack-em service-eit az alábbi docker-compose.yml fájl tartalmazza:\nversion: \u0026#39;3\u0026#39; volumes: grafana-data: driver: local prometheus-data: driver: local loki-data: driver: local services: grafaana: image: grafana/grafana:latest container_name: grafana depends_on: - prometheus - loki ports: - \u0026#34;3000:3000\u0026#34; volumes: - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources - grafana-data:/var/lib/grafana restart: unless-stopped prometheus: image: prom/prometheus:latest container_name: prometheus command: --web.enable-lifecycle --config.file=/etc/prometheus/prometheus.yml ports: - \u0026#34;9090:9090\u0026#34; volumes: - ./prometheus:/etc/prometheus - prometheus-data:/prometheus restart: unless-stopped loki: image: grafana/loki:latest container_name: loki depends_on: - promtail command: -config.file=/etc/loki/local-config.yaml ports: - \u0026#34;3100:3100\u0026#34; volumes: - loki-data:/loki restart: unless-stopped promtail: image: grafana/promtail:latest container_name: promtail command: -config.file=/etc/promtail/config.yml volumes: - ./promtail/promtail-config.yaml:/etc/promtail/config.yml restart: unless-stopped Grafana service A Grafana service-nek a ./grafana/provisioning/datasources könyvtárat, mint volume-ot állítottam be, ezen könyvtárban tárolom a használni kívánt datasource-okat (loki és prometheus).\nA Loki datasource config fájlja a grafana/provisioning/datasources/loki_ds.yml:\napiVersion: 1 datasources: - name: Loki acess: proxy type: loki url: http://loki:3100 A Prometheus datasource config fájlja a grafana/provisioning/datasources/prometheus_ds.yml:\napiVersion: 1 datasources: - name: Prometheus access: proxy type: prometheus url: http://prometheus:9090 Prometheus service A Prometheus service-nek a ./prometheus könyvtárat, mint volume-ot állítom be, hogy az itt található prometheus.yml fájlt a docker container-en kívül el lehessen érni. Ez a fájl a Prometheus config fájlja, itt lehet felvenni azon forrásokat, amelyek metric adatokat képesek szolgáltatni.\nA Prometheus indítását a command részben megadott alábbi két paraméterrel finomhangoltam:\n --config.file=/etc/prometheus/prometheus.yml: Itt adjuk át Promethusnak, hogy hol találja a használni kívánt config fájlt. --web.enable-lifecycle: Lehetőséget biztosít a prometheus.yml config fájl újrabeolvasására a /-/reload végponton, vagy az alábbi paranccsal:  curl -X POST http://localhost:9090/-/reload A Prometheus dokumentációjában azt olvashatjuk, hogy alapbeállításként 15 napig őrzi meg az adatokat, a régebbieket törli.\nA prometheus/prometheus.yml:\nglobal: scrape_interval: 5s scrape_configs: # ide sorolhatjuk fel a metric adatokat szolgáltató forrásokat Loki service Loki a log aggregáló, de a logok gyűjtését a Promtail cliens vagy agent végzi.\nPromtail service A Promtail service-nek a ./promtail/promtail-config.yaml fájlt adtam át, ez a fájl a promtail config fájlja, amelyben megadhatjuk a log forrásokat:\nserver: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: # ide sorolhatjuk fel a log forrásokat Grafana stack indítása A Grafana stack a docker-compose up -d paranccsal indítható. Miután minden elindult a grafana böngészőből a \u0026lt;server url\u0026gt;:3000 címen elérhető. A alapértelmezett felhasználó admin és jelszava admin, amit belépést követően meg kell változtatnunk.\nBelépést követően a Configuration -\u0026gt; Data sources menüpontban látnunk kell a configfájlokban megadott két datasource-unkat: Loki és Prometheus, melyeket tesztelhetünk is, hogy Grafana számára elérhetőek-e.\nA Prometheus webes felülete a \u0026lt;server url\u0026gt;:9090 címen érhető el, a Status -\u0026gt; Targets menüben jelenleg nem találunk még semmit, de amint a prometheus.yml fájlban felveszünk metric adat forrásokat, azokat itt megtekinthetjük.\nÖsszegzés Jelen bejegyzésben egy alap Grafana stack létrehozását vittem végig, a következő bejegyzésekben fogok rátérni az adatok gyűjtésére és megjelenítésére.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-05-server-monitoring-part-01-grafana-stack/","summary":"A Raspberry Pi server monitorozását a Grafana Labs eszközeivel kívánom megoldani.\nA célom a rendszer és a docker container-ek monitorozása, az adatok megjelenítése grafikonokon - úgynevezett dashboard-on -, a rendszer és docker container logok egységes kezelése, riasztások küldése nem kívánt állapotok esetén.\nA Grafana Labs főbb eszközei:\n Grafana: metric adat vizualizáló, kereső és riszasztás kezelő rendszer Prometheus: metric adatokat gyűjtő és tároló rendszer Loki: log aggregáló rendszer Promtail: agent, amely logokat továbbít Loki-nak.","title":"Raspberry Pi 3 - Part 5: Server Monitoring Part 1 - Grafana Stack"},{"content":"Docker container-ben futó alkalmazásokat nem tudunk update-elni, mivel a docker container-ek immutable-ek. Ahhoz, hogy az alkalmazás újabb verzióját használhassuk szükségünk van egy docker image-re, amely tartalmazza az új verziót és a container-ünket törölnünk kell, majd az új image-el újra létre kell hoznunk.\nWatchtower A watchtower segítségével automatizálhatjuk a docker container-jeink update-elési folyamatát.\nA watchtower monitorozza a futó Docker container-jeinket, hogy a container-ek alapjáúl szolgáló image-k változtak-e. Ha úgy érzékeli, hogy volt változás, akkor automatikusan újraindítja az érintett containereket az új image-ekre épülve, megtartva az eredeti beállításokat (portok, stb.).\nA watchtower önmaga is docker container-ben fut, az alábbi paranccsal debug módban egyszeri futtatással indíthatjuk el, tesztelve a működését:\ndocker run --name watchtower \\ -v /var/run/docker.sock:/var/run/docker.sock \\ containrrr/watchtower \\ --run-once --debug (futás után a docker container-t töröljük, nincs már szükségünk rá, mert a későbbiekben úgy hozzuk létre, hogy időzítve fusson.)\nA futás eredményeként kapott logokból láthatjuk milyen lépéseket hajt végre és ha épp aktuálisan szükséges volt, akkor update-elt is néhány container-t.\nA watchtower csak azon container-eket veszi figyelembe, amelyek alapjáúl szolgáló image-ek tegjeként a latest-et adtuk meg.\nVan lehetőségünk explicit exclude-olni container-eket, hogy egyáltalán ne is figyelje a watchtower, ennek mikéntjéről a container selection részben számolnak be.\nAuto update beállítása A watchtower container-t nem paranccsal szeretném létrehozni, hanem docker-compse-al.\nÚgy döntöttem, hogy a /opt/services könyvtárban fogom tárolni a docker-compose service fájljaimat, minden service számára külön könyvtárban.\nsudo mkdir -p /opt/services/watchtower Módosítottam a services könyvtár jogosultságát, hogy csak a tulajdonos számára legyen olvasási, írási és futtatási joga:\nsudo chmod -R 700 /opt/services Majd beállítottam a könyvtár tulajdonosának a pi user-t (ez az aktuális user-em):\nsudo chown -R pi:pi /opt/services A watchtower könyvtárban létrehoztam adocker-compose.yml fájlt, aminek a tartalma:\nversion: \u0026#34;3\u0026#34; services: watchtower: image: containrrr/watchtower container_name: watchtower command: --debug --cleanup --schedule \u0026#34;0 30 3 * * *\u0026#34; volumes: - /var/run/docker.sock:/var/run/docker.sock restart: unless-stopped A command részben adtam meg a szükséges paramétereket:\n --cleanup: régi image-ek törlése update után --schedule \u0026quot;0 30 3 * * *\u0026quot;: időzített futás beállítása minden nap 3:30-ra.  Futtatva a docker-compose up -d parancsot az időzített watchtower container elindul, a docker-compose logs paranccsal megnézhetjük a watchtower container logját, amely tartalmazza a következő futás időpontját.\nEzzel beállítottam a docker container-ek automatikus update-elését.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-04-docker-container-auto-update/","summary":"Docker container-ben futó alkalmazásokat nem tudunk update-elni, mivel a docker container-ek immutable-ek. Ahhoz, hogy az alkalmazás újabb verzióját használhassuk szükségünk van egy docker image-re, amely tartalmazza az új verziót és a container-ünket törölnünk kell, majd az új image-el újra létre kell hoznunk.\nWatchtower A watchtower segítségével automatizálhatjuk a docker container-jeink update-elési folyamatát.\nA watchtower monitorozza a futó Docker container-jeinket, hogy a container-ek alapjáúl szolgáló image-k változtak-e. Ha úgy érzékeli, hogy volt változás, akkor automatikusan újraindítja az érintett containereket az új image-ekre épülve, megtartva az eredeti beállításokat (portok, stb.","title":"Raspberry Pi 3 - Part 4: Docker Container Auto Update"},{"content":"Előfeltételek A docker telepítése előtt a Raspberry Pi 3 Model B+ esetében szükséges az alábbi cgroups beállításokkal kiegészíteni a /boot/cmdline.txt fájlt:\ncgroup_enable=memory cgroup_memory=1 cgroup_enable=cpuset swapaccount=1 A módosítást követően újra kell indítsuk a pi-t, hogy érvényesüljenek a kernel módosítások:\nsudo reboot Újraindulást követően a sudo cat /proc/cgroups eredményeként látnunk kell a cpuset és memory-t a listán.\nDocker telepítése Követve a hivatalos dokumentációt Raspbian esetén a convenience script segítségével telepíthetjük a Docker Engine-t.\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Ezt követően ellenőrizzük le, hogy minden rendben van-e a docker-rel:\ndocker info Ha minden rendben van, akkor sok információt kapunk a docker-ről és nem lesz hiba vagy warning.\nDocker kezelése non-root felhasználóval A docker parancs csak root jogosúltsággal használható, azaz csak a root userrel, vagy a sudo paranccsal.\nNon-root user esetén, hogy a sudo használatát mellőzhessem ismét a hivatalos leírást követtem:\n Egy docker group létrehozása:  sudo groupadd docker A user felvétele a docker group-ba:  sudo usermod -aG docker $USER Log out és log in, hogy a group membership újra kiértékelődjön. A group paranccsal leellenőrizhetjük, hogy a user megkapta-e a docker group-ot:  groups | grep docker Hello World container futtatása A Hello World container futtatásával tesztelhetjük, hogy a Docker telepítés sikeres volt-e:\ndocker run hello-world Docker log beállítások A post-installation steps közül a logging driver beállítását találtam még hasznosnak, mert alap esetben a docker container-ek log méretének nincs határ szabva.\nAz alapértelmezett logging driver a json-file. Ha bekapcsoljuk a log rotation-t, akkor a végtelen log problémát elkerülhetjük.\nAz /etc/docker/ könyvtárba hozzunk létre egy daemon.json fájlt a következő tartalommal:\n{ \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34; } } Így containerenként maximum 30 MB logunk lesz.\nAhhoz, hogy érvényesüljenek a beállítások újra kell indítani a Docker-t:\nsudo systemctl restart docker Továbbá csak újonnan létrehozott container-ek esetében érényesül az új logging beállítás.\nEz nem egy szükséges beállítás, mert minden container esetében megadható a használni kívánt logging-driver, illetve ott azt testre is szabhatjuk.\nDocker Compose telepítése Docker Compose telepítése pip segítségével, ha nincs python3 és pip3 telepítve, akkor azokat előbb fel kell tennünk:\nsudo apt-get install -y libffi-dev libssl-dev sudo apt-get install -y python3-dev sudo apt-get install -y python3 python3-pip Majd telepíthetjük a docker-compose-t:\nsudo pip install docker-compose ","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-03-install-docker/","summary":"Előfeltételek A docker telepítése előtt a Raspberry Pi 3 Model B+ esetében szükséges az alábbi cgroups beállításokkal kiegészíteni a /boot/cmdline.txt fájlt:\ncgroup_enable=memory cgroup_memory=1 cgroup_enable=cpuset swapaccount=1 A módosítást követően újra kell indítsuk a pi-t, hogy érvényesüljenek a kernel módosítások:\nsudo reboot Újraindulást követően a sudo cat /proc/cgroups eredményeként látnunk kell a cpuset és memory-t a listán.\nDocker telepítése Követve a hivatalos dokumentációt Raspbian esetén a convenience script segítségével telepíthetjük a Docker Engine-t.","title":"Raspberry Pi 3 - Part 3: Install docker and docker-compose"},{"content":"Debian alapú rendszerek esetén az unattended-upgrades csomag biztosít lehetőséget update-ek automatikus futtatására.\nTelepítés sudo apt-get install unattended-upgrades apt-listchanges -y Beállítások Az unattended-upgrades beállításait az /etc/apt/apt.conf.d/50unattended-upgrades fájl tartalmazza.\nLehetőségünk van beállítani az automatikus újraindítást, ha az update után az szükséges:\nUnattended-Upgrade::Automatic-Reboot \u0026quot;true\u0026quot;; Unattended-Upgrade::Automatic-Reboot-Time \u0026quot;03:00\u0026quot;; Továbbá lehetőségünk van e-mail értesítést is küldeni az alábbi két sorral:\nUnattended-Upgrade::Mail \u0026quot;user@example.com\u0026quot;; Unattended-Upgrade::MailReport \u0026quot;on-change\u0026quot;; De ez csak akkor fog működni, ha a rendszer tud e-mail-t küldeni, ehhez pedig egy olyan tool-ra van szükségünk, ami a mailx package-t támogatja.\nJelenleg úgy döntöttem e-mail küldésre nincs szükségem.\nAktiválás Az unattended-upgrades aktiválásához az alábbi parancsot futtassuk:\nsudo dpkg-reconfigure -plow unattended-upgrades Ezen parancs hatására keletkezik az /etc/apt/apt.conf.d/20auto-upgrades fájl.\nA default beállításokat meghagytam, csak kiegészítettem egy továbbival, aminek hatására 21 naponta auto clean is fut.\nAPT::Periodic::Update-Package-Lists \u0026quot;1\u0026quot;; APT::Periodic::Unattended-Upgrade \u0026quot;1\u0026quot;; APT::Periodic::AutocleanInterval \u0026quot;21\u0026quot;; További beállítási lehetőségeket az alábbi helyekről lehet kinézni:\n /etc/cron.daily/apt-compat /usr/lib/apt/apt.systemd.daily  Végül pedig dry-run módban (ami csak szimulálja a futást, de nem telepít) és debug infokkal ellátva futtassuk az unattended-upgrade-t, hogy megfelel-e elvárásainknak:\nsudo unattended-upgrade --dry-run --debug A futások logját az alábbi helyen találjuk: /var/log/unattended-upgrades/unattended-upgrades.log\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-02-auto-update/","summary":"Debian alapú rendszerek esetén az unattended-upgrades csomag biztosít lehetőséget update-ek automatikus futtatására.\nTelepítés sudo apt-get install unattended-upgrades apt-listchanges -y Beállítások Az unattended-upgrades beállításait az /etc/apt/apt.conf.d/50unattended-upgrades fájl tartalmazza.\nLehetőségünk van beállítani az automatikus újraindítást, ha az update után az szükséges:\nUnattended-Upgrade::Automatic-Reboot \u0026quot;true\u0026quot;; Unattended-Upgrade::Automatic-Reboot-Time \u0026quot;03:00\u0026quot;; Továbbá lehetőségünk van e-mail értesítést is küldeni az alábbi két sorral:\nUnattended-Upgrade::Mail \u0026quot;user@example.com\u0026quot;; Unattended-Upgrade::MailReport \u0026quot;on-change\u0026quot;; De ez csak akkor fog működni, ha a rendszer tud e-mail-t küldeni, ehhez pedig egy olyan tool-ra van szükségünk, ami a mailx package-t támogatja.","title":"Raspberry Pi 3 - Part 2: Auto update"},{"content":"Az OS telepítése A célom hogy headless módon használhassam a pi-t, így nincs szükségem desktop enviromentre. Követve a hivatalos getting started leírást, az OS-t a Raspberry Pi Imager segítségével telepítem.\nMiután az SD kártyát a számítógépemhez csatlakoztattam az RP Imager-ben OS-ként nem a default beállítást választottam, hanem az other opció alatt elérhető DE nélküli változatot (ebből két verzió létezik: Lite és Full, az utóbbit választottam).\nAz Advanced Options segítségével be tudtam állítani a wifi-t, a locale beállításokat és az ssh-t.\nAz alapos előkészületeket követően, megnyomtam a WRITE gombot és némi idő elteltével be is fejeződött a telepítés.\nTelepítést követően Az SD kártyát a pi-be helyezve, majd az áram alá téve hamarosan el is volt érhető a hálózaton. Besshztam a telpítés előtti advancen options-ben beállított user és password segítsévégével.\nElső parancsommal a rendszert frissítettem:\nsudo apt update -y \u0026amp;\u0026amp; sudo apt full-upgrade -y Ennek végeztével el is kezdődhet a kaland.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-01-setup/","summary":"Az OS telepítése A célom hogy headless módon használhassam a pi-t, így nincs szükségem desktop enviromentre. Követve a hivatalos getting started leírást, az OS-t a Raspberry Pi Imager segítségével telepítem.\nMiután az SD kártyát a számítógépemhez csatlakoztattam az RP Imager-ben OS-ként nem a default beállítást választottam, hanem az other opció alatt elérhető DE nélküli változatot (ebből két verzió létezik: Lite és Full, az utóbbit választottam).\nAz Advanced Options segítségével be tudtam állítani a wifi-t, a locale beállításokat és az ssh-t.","title":"Raspberry Pi 3 - Part 1: Setup"},{"content":"A Docker Compose egy olyan eszköz, aminek segítségével definiálni és elindítani tudunk több docker container-t.\nDocker Compose fájlok A docker-compose.yml fájl tartalmazza azon service-eket amelyeket docker container-ként akarunk futtatni.\nA .env fájl nem kötelező, de ha használjuk, akkor környezeti változókat tartalmazhat, amelyek a docker-compose.yml fájlban behivatkozhatóak. További részletek itt találhatóak.\nRendelkezésre állnak előre definiált környezeti változók, amelyekkel finomhangolhatjuk a Docker Compose működését. Az egyik ilyen környezeti változó a COMPOSE_PROJECT_NAME, amellyel a projektünk nevét adhatjuk meg, amit a docker-compose utána felhasznál prefixként a létrehozandó container-ek, volume-ok, network-ok nevéhez.\nAz elérhető környezeti változók megtalálhatóak a Compose CLI environment variables oldalon.\nDocker Compose kezelése Definíció ellenőrzése Miután elkészítettük a docker-compose.yml fájl és esetleg a .env fájl, úgy az alábbi paranccsal leellenőrizhetjük azok helyességét:\ndocker-compose config Service-k indítása docker-compose up -d A -d kapcsolóval háttérfolyamatként indítja el a service-ket.\nFutó service-ek megtekintése docker-compose ps Futó servvice-k log-jának megtekintése docker-compose logs -f A -f kapcsoló követi a log-ot.\nFutó service-ek leállítása docker-compose stop Service-k törlése docker-compose down Ha futnak a service-ek akkor azok leállításra kerülnek, majd a container-ek törlése. Ha a -v kapcsolót is megadjuk, akkor a volume-ok is törlésre kerülnek.\n","permalink":"https://thomastrinn.github.io/blog/posts/get-started-with-docker-compose/","summary":"A Docker Compose egy olyan eszköz, aminek segítségével definiálni és elindítani tudunk több docker container-t.\nDocker Compose fájlok A docker-compose.yml fájl tartalmazza azon service-eket amelyeket docker container-ként akarunk futtatni.\nA .env fájl nem kötelező, de ha használjuk, akkor környezeti változókat tartalmazhat, amelyek a docker-compose.yml fájlban behivatkozhatóak. További részletek itt találhatóak.\nRendelkezésre állnak előre definiált környezeti változók, amelyekkel finomhangolhatjuk a Docker Compose működését. Az egyik ilyen környezeti változó a COMPOSE_PROJECT_NAME, amellyel a projektünk nevét adhatjuk meg, amit a docker-compose utána felhasznál prefixként a létrehozandó container-ek, volume-ok, network-ok nevéhez.","title":"Get Started With Docker Compose"},{"content":"Az alábbi docker-compose.yml fájl egy elasticsearch és egy elastickibana service-t indít el.\nversion: \u0026#39;3\u0026#39; services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.9.3 container_name: ebla_elasticsearch environment: - xpack.security.enabled=false - discovery.type=single-node ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 cap_add: - IPC_LOCK volumes: - elasticsearch_data:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 kibana: image: docker.elastic.co/kibana/kibana:7.9.3 container_name: ebla_kibana environment: - ELASTICSEARCH_HOSTS=http://elasticsearch:9200 ports: - 5601:5601 depends_on: - elasticsearch volumes: elasticsearch_data: driver: local Ezen beállítások development környezetre ajánlatosak, production esetén ez kerülendő. További információk itt találhatóak.\n","permalink":"https://thomastrinn.github.io/blog/posts/elasticsearch-in-docker/","summary":"Az alábbi docker-compose.yml fájl egy elasticsearch és egy elastickibana service-t indít el.\nversion: \u0026#39;3\u0026#39; services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.9.3 container_name: ebla_elasticsearch environment: - xpack.security.enabled=false - discovery.type=single-node ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 cap_add: - IPC_LOCK volumes: - elasticsearch_data:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 kibana: image: docker.elastic.co/kibana/kibana:7.9.3 container_name: ebla_kibana environment: - ELASTICSEARCH_HOSTS=http://elasticsearch:9200 ports: - 5601:5601 depends_on: - elasticsearch volumes: elasticsearch_data: driver: local Ezen beállítások development környezetre ajánlatosak, production esetén ez kerülendő.","title":"Elasticsearch in Docker"},{"content":"A SonarQube egy kód minőség elemző eszköz.\nSonarQube futtatása docker-compose service-ként docker-compose.yml:\nversion: \u0026#34;3\u0026#34; services: sonarqube: image: sonarqube:8.9-community container_name: sonarqube depends_on: - db ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 environment: SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar SONAR_JDBC_USERNAME: sonar SONAR_JDBC_PASSWORD: sonar volumes: - sonarqube_data:/opt/sonarqube/data - sonarqube_extensions:/opt/sonarqube/extensions - sonarqube_logs:/opt/sonarqube/logs ports: - \u0026#34;9010:9000\u0026#34; db: image: postgres:12 container_name: sonarqube-postgres environment: POSTGRES_USER: sonar POSTGRES_PASSWORD: sonar ports: - 14432:5432 volumes: - postgresql:/var/lib/postgresql - postgresql_data:/var/lib/postgresql/data volumes: sonarqube_data: sonarqube_extensions: sonarqube_logs: postgresql: postgresql_data: SonarQube indítása: docker-compose up -d\nMivel a SonarQube embedded Elasticserch-t használ szükség lehet a host gépen a következő beállításra:\nsysctl -w vm.max_map_count=524288 Ha még így is gond volna, akkor a SonarQube Docker Hub oldalból induljál ki.\nSikeres indítás követően az alkalmazás elérhető a http://localhost:9010/ url-en.\nBelépni admin/admin segítségével lehet.\nSonar Scanner futtatása docker container-ből Ha sonar-scanner-t kell futtatnunk a projektünk forráskódján, akkor azt is megtehetjük docker container-ből, az alábbi módon:\ndocker run --rm \\ --net host \\ -v \u0026quot;$(pwd):/usr/src\u0026quot; \\ sonarsource/sonar-scanner-cli \\ -Dsonar.projectKey=\u0026lt;your-projectKey\u0026gt; \\ -Dsonar.host.url=\u0026lt;your-sonar-host\u0026gt; \\ -Dsonar.login=\u0026lt;your-login\u0026gt; A -v \u0026quot;$(pwd):/usr/src\u0026quot; azt a mapping-et mutatja, hogy ha a forráskódunk könyvtárában adjuk ki ezt a parancsot, akkor az a container /usr/src könyvtárára mutat, amelyben a sonar-scanner keresi a forráskódot.\nA -Dsonar.* paraméterek értékét meg a sonarqube oldal a project létrehozásakor megad nekünk, s azokat kell ide behelyettesítenünk.\n","permalink":"https://thomastrinn.github.io/blog/posts/sonarqube-in-docker/","summary":"A SonarQube egy kód minőség elemző eszköz.\nSonarQube futtatása docker-compose service-ként docker-compose.yml:\nversion: \u0026#34;3\u0026#34; services: sonarqube: image: sonarqube:8.9-community container_name: sonarqube depends_on: - db ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 environment: SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar SONAR_JDBC_USERNAME: sonar SONAR_JDBC_PASSWORD: sonar volumes: - sonarqube_data:/opt/sonarqube/data - sonarqube_extensions:/opt/sonarqube/extensions - sonarqube_logs:/opt/sonarqube/logs ports: - \u0026#34;9010:9000\u0026#34; db: image: postgres:12 container_name: sonarqube-postgres environment: POSTGRES_USER: sonar POSTGRES_PASSWORD: sonar ports: - 14432:5432 volumes: - postgresql:/var/lib/postgresql - postgresql_data:/var/lib/postgresql/data volumes: sonarqube_data: sonarqube_extensions: sonarqube_logs: postgresql: postgresql_data: SonarQube indítása: docker-compose up -d","title":"SonarQube in Docker"},{"content":"PostgreSQL server könnyen indítható docker container segítségével. A container létrehozását docker-compose segítségével végzem.\nPostgreSQL docker-compose service Ahhoz, hogy docker-compose segítségével hozzunk létre a postgresql docker containert két fájlra lesz szükségünk. Az egyik a .env fájl a másik a docker-compose.yml.\nA .env fájl nem kötelező, de praktikus, itt környezeti változókat definiálhatunk, amelyeket a docker-compose.yml fájlban fel tudunk használni.\nA docker-compose.yml fájlban definiáljuk a service-ket a volumes-eket. Jelen esetben csak egy service-t definiálunk, a postgresql-t.\nA .env fájl tartalma:\nCOMPOSE_PROJECT_NAME=postgres POSTGRES_PASSWORD=postgres POSTGRES_USER=postgres POSTGRES_DB=postgres A COMPOSE_PROJECT_NAME környezeti változóval a project nevét adhatjuk meg, részleteket itt olvashatunk.\nA többi környezeti változót pedig a postgresql service fogja felhasználni.\nA docker-compose.yml fájl tartalma:\nversion: \u0026#39;3.8\u0026#39; services: postgres: image: postgres:12 container_name: postgres environment: - POSTGRES_USER=${POSTGRES_USER} - POSTGRES_PASSWORD=${POSTGRES_PASSWORD} - POSTGRES_DB=${POSTGRES_DB} command: postgres -c max_connections=150 volumes: - postgres:/var/lib/postgresql/data ports: - 5432:5432 volumes: postgres: driver: local A .env-ben definiált környezeti változókat itt az enviroment: szakaszban használjuk fel, adjuk át a container-nek induláskor.\nEgy postgres nevű volume-ot definiálunk, ami a container-ben levő /var/lib/postgresql/data könyvtárra mutat. Így azét érjük el, hogy a container törlése után a postgres adatok megmaradnak és a container újra létrehozását követően meglesznek az adatbázis adatok.\nA container-ben futó postgre adatbázis beállítását a command: résznél végezhetjük el, a postgres -c \u0026lt;param1=value1\u0026gt; -c \u0026lt;param2=value2\u0026gt; parancsot megadva. A fenti esetben a max_connections értékét adjuk meg (alap értéke 100).\nKiterjedtebb leírást a postgres dockerhub oldalon találunk.\nPostgreSQL tuning Egy adott környezetre a beállítások testreszabása érdekében a következő két forrást tudom javasolni:\n Tuning your PostgreSQL Server: részletes leírása az egyes beállításoknak PGTune: egyszerű eszköz amivel könnyedén előállíthatjuk a config értékeket.  ","permalink":"https://thomastrinn.github.io/blog/posts/postgresql-in-docker/","summary":"PostgreSQL server könnyen indítható docker container segítségével. A container létrehozását docker-compose segítségével végzem.\nPostgreSQL docker-compose service Ahhoz, hogy docker-compose segítségével hozzunk létre a postgresql docker containert két fájlra lesz szükségünk. Az egyik a .env fájl a másik a docker-compose.yml.\nA .env fájl nem kötelező, de praktikus, itt környezeti változókat definiálhatunk, amelyeket a docker-compose.yml fájlban fel tudunk használni.\nA docker-compose.yml fájlban definiáljuk a service-ket a volumes-eket. Jelen esetben csak egy service-t definiálunk, a postgresql-t.","title":"PostgreSQL in Docker"},{"content":"Journal mérete A journal a /var/log/journal/ könyvtárba hozza létre a log fájlokat, amik idővel sok helyet foglalhatnak. Ennek határt szabhatunk.\nA /etc/systemd/journald.conf fájlban a SystemMaxUse változónak értéket adhatunk:\nSystemMaxUse=50M A config fájl módosítását követően a journal service-t újra kell indítanunk, hogy a változtatások érvényesüljenek:\nsystemctl restart systemd-journald Hibák megtekintése Az alábbi paranccsal a log-ban levő hibákat tekinthetjük meg:\njournalctl -r -p 3 További részletek: arch wiki journal\n","permalink":"https://thomastrinn.github.io/blog/posts/linux-systemd-journal/","summary":"Journal mérete A journal a /var/log/journal/ könyvtárba hozza létre a log fájlokat, amik idővel sok helyet foglalhatnak. Ennek határt szabhatunk.\nA /etc/systemd/journald.conf fájlban a SystemMaxUse változónak értéket adhatunk:\nSystemMaxUse=50M A config fájl módosítását követően a journal service-t újra kell indítanunk, hogy a változtatások érvényesüljenek:\nsystemctl restart systemd-journald Hibák megtekintése Az alábbi paranccsal a log-ban levő hibákat tekinthetjük meg:\njournalctl -r -p 3 További részletek: arch wiki journal","title":"Linux Systemd Journal"}]