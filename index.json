[{"content":"Az Alpine Linux upgrade-je a legutóbbi release-re igen egyszerű folyamat, amely jól dukomentált is az alpine wiki oldalon.\nAz upgrade folyamatot annyiban egészíteném ki, hogy biztonság kedvéért az aktuális verzión frissítsük be a rendszerünket.\nAz aktuálisan használt verzió lekérése:\ncat /etc/alpine-release Új release-re upgrade előtt érdemes upgrade-elni az aktuális verziónkat:\nsudo apk -U upgrade Most, hogy előkészültünk megléphetjük, hogy az /etc/apk/repositories fájlban lecseréljük a használt repository-kban a verziószámot a kívánt új release számára, vagy akár a latest-stable kulcsszóra, hogy mindig a legutóbbi release-t használjuk.\nEzt követően jöhet az upgrade:\napk update apk upgrade --available Végül pedig ne feledjük el, hogy minden service, ami upgrade-elve lett, azt újra kell indítanunk, illetve ha a kernel is upgrade-elve lett, akkor újra kell indítanunk a gépünket:\nsudo sync sudo reboot ","permalink":"https://thomastrinn.github.io/blog/posts/alpine-upgrade-to-latest-release/","summary":"Az Alpine Linux upgrade-je a legutóbbi release-re igen egyszerű folyamat, amely jól dukomentált is az alpine wiki oldalon.\nAz upgrade folyamatot annyiban egészíteném ki, hogy biztonság kedvéért az aktuális verzión frissítsük be a rendszerünket.\nAz aktuálisan használt verzió lekérése:\ncat /etc/alpine-release Új release-re upgrade előtt érdemes upgrade-elni az aktuális verziónkat:\nsudo apk -U upgrade Most, hogy előkészültünk megléphetjük, hogy az /etc/apk/repositories fájlban lecseréljük a használt repository-kban a verziószámot a kívánt új release számára, vagy akár a latest-stable kulcsszóra, hogy mindig a legutóbbi release-t használjuk.","title":"Alpine: upgrade to latest release"},{"content":"Az év elején vágtam bele a home lab server projektbe és operációs rendszernek az Ubuntu Server-t választottam. A döntésről és beállításokról írt bejegyzésemben többek közt több lehetséges operációs rendszert is felsoroltam, amelyen elgondolkoztam, hogy használhatnám, de az ubuntu popularitása és egyszerű telepítése miatt végül azokat elvetettem.\nNéhány hónap elteltével azonban elkezdett motoszkálni a fejemben, hogy nekem tényleg szükségem van-e az ubuntura, kell-e nekem a sok csomag amivel jön, kell-e nekem a fix verzió, kell-e nekem a régebbi kernel (és nem egy újabb). Nincs-e másik os, ami a kis home lab server-em számára elégséges: kicsi erőforrás, kevés előtelepített csomaggal. Mivel a fő célom, hogy containerizált környezetben futtassam a használt szolgáltatásokat, valami minimális számomra elégséges lehet. És ez volt az a pont, amikor elgondolkoztam az arch linux használatán. De valahogy nem vonzott be és elkezdtem szemezni az alpine linux-szal. Az alpine kimondottan kis erőforrás igényű, minimális linux. És úgy gondoltam ez kell nekem.\nAlpine linux telepítése Be kell valljam, elsőre úgy éreztem a parancssori telepítés nehéz lesz, sok hiba lehetőséget rejt, de meglepően jó élmény volt.\nAz alpine linux-nak több változata van, közülük kettő jöhetett számomra szóba:\nSTANDARD: Alpine as it was intended. Just enough to get you started. Network connection is required.\nEXTENDED: Most common used packages included. Suitable for routers and servers. Runs from RAM. Includes AMD and Intel microcode updates.\nMivel az extended változatnál kiemelték, hogy szerverek számára alkalmas így emellett döntöttem.\nA telepítővel való ismerkedés céljából elsőként virtuális gépen próbáltam ki és kellemes meglepetésként ért, hogy van automatikus telepítésre lehetőség, ami annyit tesz, hogy boot-olva a telepitőről egy parancs kiadásával feltelepíthetjük a rendszert:\nsetup-alpine -q Így hamar eljutottam egy használható rendszerhez. Természetesen a telepítés alapértékekkel történt, ami nem biztos, hogy mindenki számára megfelelő, de arra a célra pont megfelelő volt, hogy magával a rendszerrel elkezdhessek ismerkedni.\nMiután úgy éreztem megvan a potenciál benne, hogy használjam a szerveren, megnéztem, hogy tudom egyéni beállításokkal telepíteni, erre a semi-automatic telepítési leírás adja meg a választ. Így hát újabb virtuális gépen való telepítésbe vágtam bele. A semi-automatic telepítési leírás nagyon jó, figyelmes olvasással mindent meg tudtam csinálni, így elérkezettnek éreztem az időt, hogy akkor mehet a vasra.\nAz alábbiakban kitérnék azon részekre, ahol némileg több energiát kellett befektetnem a telepítésnél.\nNetworking - DHCP Alapvetően a setup-interface -a (az \u0026ldquo;a\u0026rdquo; az automatikusat jelenti) paranccsal lehet beállítani a networking-et és a dhcp-t, de mivel a kis gépben ethernet és wifi is van, manuális setup-ra volt szükségem, amit a setup-interface paranccsal lehet indítani. Itt lényegében annyit kellett tennem, hogy kiválasztottam melyik interface-t akarom használni (az ethernet-et választottam) és annak beállítását már automatikusan utána megtette.\nPackage Repositories Itt a setup-apkrepos paranccsal le lehet kérni az elérhető repository listákat és lehetőségünk van választani egyet, ehhez segítség, hogy választhatjuk azt amelyik a leggyorsabb, ehhez a \u0026ldquo;find fastets\u0026rdquo; opció szolgál.\nSetup disk Ez volt számomra a legnehezebb rész. A kis gépben van egy SSD és egy HDD. Az SSD-re szeretném telepíteni az operációs rendszert, UEFI-t szeretnék használni és grub-ot bootloader-nek, a lemezt ext4-re akarom formázni és nem akarok swap partíciót.\nÉs mindenre van lehetőségem a setup-disk paranccsal:\nUSE_EFI=true setup-disk -s 0 -m sys /dev/nvme0n1 USE_EFI=ture: ezen környezeti változóval állítottam be, hogy UEFI-t akarok használni\n-s 0: a swap partíció méretének 0-t adtam meg, így nem lesz swap partíció\n-m sys: lényegében, hogy lemezre akarom telepíteni és készítse el a partíciót(kat) és formázza meg a lemezt\n/dev/nvme0n1: az SSD, amelyre telepíteni kívánom a rendszert\nnincs megadva, de default ext4 fájlrendszer lesz használa a /-en, ha módosítani akarnám, akkor a ROOTFS környezeti változóval tehetném meg (lehetséges értékek: ext2, ext3, ext4, (flat) btrfs és xfs)\nTelepítés utáni teendők A telepítés után még néhány dolgot meg kell tenni, hogy használhassuk a rendszert:\nnormal user létrehozása\nadminisztrációs jog kiosztása normal user-hez\nEzeket a telepítés utáni lépéseket jól dokumentálták.\nEzen felül még az alábbiakat tettem meg:\nA HDD mount-olása /data-ra; mountolásról ezen bejegyzésemben írtam\ndocker és docker-compose telepítése\nA docker servicek számára könyvtár létrehozása, adott felhasználó legyen a tulajdonosa:\nsudo mkdir -p /opt/services \u0026amp;\u0026amp; sudo chown $USER /opt/services Összegzés Úgy érzem jó döntés volt váltani alpine linux-ra. Nem kell mindig a legelterjedtebb eszközt, módszert használni.\n","permalink":"https://thomastrinn.github.io/blog/posts/home-lab-server-setup-2/","summary":"Az év elején vágtam bele a home lab server projektbe és operációs rendszernek az Ubuntu Server-t választottam. A döntésről és beállításokról írt bejegyzésemben többek közt több lehetséges operációs rendszert is felsoroltam, amelyen elgondolkoztam, hogy használhatnám, de az ubuntu popularitása és egyszerű telepítése miatt végül azokat elvetettem.\nNéhány hónap elteltével azonban elkezdett motoszkálni a fejemben, hogy nekem tényleg szükségem van-e az ubuntura, kell-e nekem a sok csomag amivel jön, kell-e nekem a fix verzió, kell-e nekem a régebbi kernel (és nem egy újabb).","title":"Home Lab Server: choose OS 2.0"},{"content":"Szükség van-e swap-ra? Erre a kérdésre kerestem a választ és találtam egy jó cikket, amely ezt a témát járja körbe: Linux Performance: Why You Should Almost Always Add Swap Space\nAz alábbiakban leírom, hogyan tudunk swap fájlt létrehozni és milyen finomhangolási lehetőségeink vannak.\nSwap fájl létrehozása és használata Swapfájl létrehozása. Például 512 mb-os swapfájl létrehozása:\nsudo dd if=/dev/zero of=/swapfile bs=1M count=512 status=progress A dd a coreutils GNU csomag része.\nMegfelelő jogosultságokat kell adnunk a fájlnak. Csak a root felhasználó számára lehet olvasható és írható. Ez a következőképp tehető meg:\nsudo chmod 600 /swapfile A fájlt formáznunk kell:\nsudo mkswap /swapfile Most már engedélyezhetjük, használhatjuk swapfájlként:\nsudo swapon /swapfile Végül pedig, hogy boot során is engedélyezett legyen fel kell vennünk az /etc/fstab fájlba:\nsudo bash -c \u0026#34;echo /swapfile none swap defaults 0 0 \u0026gt;\u0026gt; /etc/fstab\u0026#34; Ez a módszer BTRFS és ZFS fájlrendszer esetén nem fog működni.\nFinomhangolás Két paraméter segítségével finomhangolhatjuk a rendszer memória és swap kezelését:\nswappiness: Ezzel szabályozható, hogy mekkora valószínűséggel kerüljön át egy page a swap-ra. Ez az érték a szabad memória százalékos arányát jelenti mielőtt aktiválódik a swap. Az alapértelmezett érték 60, ami ideális szerverek esetén, asztali számítógépek esetén az ajánlott érték 10.\nvfs_cache_pressure: Ez egy százalékos érték, amely szabályozza a kernel azon tendenciáját, hogy visszafoglalja a könyvtár- és inode objektumok cache-elésére használt memóriát. Alapértéke 100 a 5.4-es kernel verzió előtt. Az 5.4 vagy újabb kernelek esetén értéke növelhető 100 fölé. Ezen érték növelése növeli az objektumok eltávolításának mértékét a memória cache-ből. Ennek csökkentése lehetővé teszi, hogy ezek az objektumok hosszabb ideig legyenek cache-elve a memóriában. Az ajánlott érték 50-től 200-ig tartományban.\nAmennyiben elegendő memória áll rendelkezésünkre, akár szerver akár személyi számítógép esetén a következő beállítások megfelelőek lehetnek:\nvm.swappiness=10 vm.vfs_cache_pressure=50 Ezen változók aktuális értékének lekérdezése az alábbi módon tehető meg:\nsudo cat /proc/sys/vm/swappiness sudo cat /proc/sys/vm/vfs_cache_pressure Ezen értékek ideiglenes beállítása:\nsudo sysctl -w vm.swappiness=10 sudo sysctl -w vm.vfs_cache_pressure=50 Ahhoz, hogy permanenssé tegyük a beállítást egy sysctl.d config fájlt kell létrehoznunk. Például hozzunk létre egy /etc/sysctl.d/99-swap.conf fájlt:\nvm.swappiness=10 vm.vfs_cache_pressure=50 sysctl.d - Configure kernel parameters at boot\nSwap fájl törlése Swap fájl törlése előtt először ki kell kapcsolni a swap fájl használatát:\nswapoff /swapfile rm -f /swapfile És végül törölni kell az /etc/fstab fájlból a hozzá tartozó bejegyzést.\n","permalink":"https://thomastrinn.github.io/blog/posts/swap/","summary":"Szükség van-e swap-ra? Erre a kérdésre kerestem a választ és találtam egy jó cikket, amely ezt a témát járja körbe: Linux Performance: Why You Should Almost Always Add Swap Space\nAz alábbiakban leírom, hogyan tudunk swap fájlt létrehozni és milyen finomhangolási lehetőségeink vannak.\nSwap fájl létrehozása és használata Swapfájl létrehozása. Például 512 mb-os swapfájl létrehozása:\nsudo dd if=/dev/zero of=/swapfile bs=1M count=512 status=progress A dd a coreutils GNU csomag része.\nMegfelelő jogosultságokat kell adnunk a fájlnak.","title":"Swap"},{"content":"Linux és UNIX operációs rendszerek esetén a mount parancs segítségével csatolhatunk fel fájlrendszereket. A felcsatolás helyét mount point-nak nevezik és a könyvtár struktúra egy tetszőlegesen választott helye.\nAz unmount parancs segítségével pedig lecsatolható az adott fájlrendszer.\nFájlrendszer csatolása Az elérhető eszközök listázására használhatjuk az fdisk parancsot:\nsudo fdisk -l Ezzel részletes információkat kapunk az eszközökről és a partíciókról, illetve milyen fájlrendszerűek a partíciók.\nAdott fájlrendszer csatolásának módja a mount paranccsal:\nmount [OPTION...] DEVICE_NAME DIRECTORY Konkrét példával, ha a /dev/sdb1 fájlrendszert akarjuk a /mnt/media könyvtárba csatolni, akkor azt a következőképp tehetjük:\nsudo mount /dev/sdb1 /mnt/media Amennyiben gyakori fájlrendszert, például ext4 vagy xfs csatolunk, akkor azt a mount fel tudja ismerni. Egyébként explicit meg kell határoznunk a fájlrendszer típusát. Ehhez a -t kapcsolót használhatjuk:\nmount -t TYPE DEVICE_NAME DIRECTORY További lehetőségek a -o mount options kapcsolók segítségével adhatóak meg:\nmount -o OPTIONS DEVICE_NAME DIRECTORY Fájlrendszer csatolása /etc/fstab segítségével Az /etc/fstab konfigurációs fájl tartalmazza a fájlrendszerek csatolásához szükséges információkat, ennek segítségével automatikussá tehető a fájlrendszerek csatolása.\nA fájl tartalma több sor, melyek szerkezete az alábbi:\n[Device] [Mount Point] [File System Type] [Options] [Dump] [Pass] Pontos és részletes leírás az fstab-ról az ubuntu wiki Fstab oldalán található.\nEgy gyakori beállítás:\nUUID=\u0026lt;uuid\u0026gt; \u0026lt;pathtomount\u0026gt; \u0026lt;filesystem\u0026gt; defaults 0 0 Érdemes device-ként annak UUID -ját használni, amit az lsblk parancs segítségével tudhatunk meg:\nsudo lsblk -o NAME,FSTYPE,UUID Az UUID használata azért előnyös, mert ha a device neve változik is, attól függetlenül meg tud történni az eszköz csatolása.\nFontos a [PASS] értékének helyes megválasztása, mert ez határozza meg az fsck sorrendjét. Ennek részletei is megtalálhatóak a fenti ubuntu wiki linken.\n","permalink":"https://thomastrinn.github.io/blog/posts/mounting-a-file-system/","summary":"Linux és UNIX operációs rendszerek esetén a mount parancs segítségével csatolhatunk fel fájlrendszereket. A felcsatolás helyét mount point-nak nevezik és a könyvtár struktúra egy tetszőlegesen választott helye.\nAz unmount parancs segítségével pedig lecsatolható az adott fájlrendszer.\nFájlrendszer csatolása Az elérhető eszközök listázására használhatjuk az fdisk parancsot:\nsudo fdisk -l Ezzel részletes információkat kapunk az eszközökről és a partíciókról, illetve milyen fájlrendszerűek a partíciók.\nAdott fájlrendszer csatolásának módja a mount paranccsal:","title":"Mounting a File System"},{"content":"A korábbi SSH túl sok autentikációs próbák hiba bejegyzést követően tudomást szereztem arról, hogy egy kapcsolódáshoz tartozó speciális beállításokat egy config fájlban is meg lehet adni, ami nagyban megkönnyíti a dolgomat, mert nem kell megjegyeznem a beállításokat.\nOpenSSH client-side configuration file: ~/.ssh/config Alap esetben az SSH config fájl nem feltétlenül létezik, ezért létre kell hozni úgy, hogy csak az adott felhasználó írhassa és olvashassa:\ntouch ~/.ssh/config \u0026amp;\u0026amp; chmod 600 ~/.ssh/config A fájl struktúrája:\n# ~/.ssh/config Host hostname1 SSH_OPTION value SSH_OPTION value Host hostname2 SSH_OPTION value Host * SSH_OPTION value A fájl tartalma stanzákra/strófákra van osztva. Minden stanza a Host direktívával kezdődik és SSH beállításokat tartalmaz, amelyek a távoli ssh szerverhez való csatlakozáshoz lesznek felhasználva.\nA fájl tartalmazhat üres sorokat. Indentálásra nincs szükség, de olvashatóság szempontjából hasznos.\nA #-al kezdődő sorok megjegyzésekként vannak kezelve.\nA Host direktíva állhat egy pattern-ből vagy szóközzel elválasztott pattern listából. A patternekről illetve a lehetséges ssh opciókról az ssh_config manualjában olvashatunk részletesebben: man ssh_config\nAz SSH kliens a config fájlt stanzáról stanzára olvassa, és ha több mint egy pattern illeszkedik, akkor az elsőnek illeszkedő stanza opciói elsőbbséget élveznek. Ez azt jelenti, hogy az illeszkedő stanzák opciói bár nem írják felül a korábbiakat, de újakat hozzáfűzhetnek. Ezért érdemes a host-specifikus beállításokat a fájl elejére helyezni és a végére az általánosabbakat.\nAz SSH config fájl más programok által is használt, mint scp, sftp és rsync.\nSSH config fájl példa Egy tipikus példa távoli szerverhez ssh-val való csatlakozásra:\nssh ubuntu@server.example.com -p 2322 Az ssh config fájl segítségével helyettesíthetjük a fentit az alábbival:\nssh server Az ehhez tartozó config:\n# ~/.ssh/config Host dev HostName server.example.com User ubuntu Port 2322 SSH config fájl opciók felülbírálása Az ssh kliens a következő precedencia sorrendben olvassa a beállításait:\nparancssorban megadott opciók\na ~/.ssh/config által meghatározott opciók\na /etc/ssh/ssh_config rendszer szintű általános opciók\nVan lehetőség alternatív config fájl megadására is a -F kapcsolóval.\nAnnak érdekében, hogy az ssh kliens ignoráljon minden beállítást a config fájlok alapján használhatjuk az alábbit:\nssh -F /dev/null user@example.com Összegzés A ~/.ssh/config fájl használata igen megegyszerűsíti az ember életét, nem kell fejben tartani a host-ok ip címét, ssh port-ot és egyéb beállításokat.\n","permalink":"https://thomastrinn.github.io/blog/posts/simplify-remote-access-with-custom-ssh-connection-configs/","summary":"A korábbi SSH túl sok autentikációs próbák hiba bejegyzést követően tudomást szereztem arról, hogy egy kapcsolódáshoz tartozó speciális beállításokat egy config fájlban is meg lehet adni, ami nagyban megkönnyíti a dolgomat, mert nem kell megjegyeznem a beállításokat.\nOpenSSH client-side configuration file: ~/.ssh/config Alap esetben az SSH config fájl nem feltétlenül létezik, ezért létre kell hozni úgy, hogy csak az adott felhasználó írhassa és olvashassa:\ntouch ~/.ssh/config \u0026amp;\u0026amp; chmod 600 ~/.ssh/config A fájl struktúrája:","title":"Simplify Remote Access with custom SSH Connection Configs"},{"content":"A mai nap, amikor be akartam ssh-zni egy szerverre, azt a szerver a következő hiba üzenettel utasította vissza:\nReceived disconnect from \u0026lt;server\u0026gt;: 2: Too many authentication failures for username Az ssh-hoz autentikációnak jelszót kellett volna megadnom, de addig el se jutottam. Az alábbi egyszerű módon kezdeményeztem a kapcsolódást:\nssh username@\u0026lt;server\u0026gt; Megnéztem a -v és -vvv kapcsolók segítségével, hogy mi is történik, hol akad el a folyamat, de számomra semmi gyanúsat nem találtam.\nSzerencsémre a világhálón körülnézve rátaláltam, hogy nem én vagyok az egyedüli ezzel a problémával.\nA probléma gyökere Az ssh folyamat során - amit a -v vagy -vvv kapcsolók segítségével magam is végig néztem, de számomra nem volt gyanús - az elérhető ssh-key-ek lesznek először felhasználva. Nekem természetesen több mint egy kulcsom volt, ami azt eredményezte, hogy egymás után sikertelen autentikációs kísérleteim voltak és ezzel elértem a szerver ssh beállításában (vagy az alapértelmezett) limitet a próbák számára.\nAz ssh szerveren ezen limitet a /etc/ssh/sshd_config fájlban az alábbi értékkel lehet módosítani:\nMaxAuthTries 3 De természetesen nem ez az igazi megoldás, helyette arról kellene gondoskodni, hogy ne történjenek meg ezen felesleges próbálkozások.\nA megoldás Ahhoz hogy ssh autentikáció során megadhassuk, hogy explicite csak jelszóval szeretnénk kezdeményezni a kapcsolódást, egy extra kapcsolót kell használnunk:\nssh -o PubkeyAuthentication=no username@\u0026lt;server\u0026gt; Abban az esetben, ha ssh-key-jel szeretnénk kapcsolódni, úgy arra is van lehetőség, hogy megadjuk melyik key-t használjuk és, hogy csak azt használja a kapcsolódáshoz, ehhez is extra kapcsolót kell használnunk:\nssh -o \u0026#39;IdentitiesOnly yes\u0026#39; -i keypair.pem username@\u0026lt;server\u0026gt; ","permalink":"https://thomastrinn.github.io/blog/posts/ssh-too-many-authentication-failures/","summary":"A mai nap, amikor be akartam ssh-zni egy szerverre, azt a szerver a következő hiba üzenettel utasította vissza:\nReceived disconnect from \u0026lt;server\u0026gt;: 2: Too many authentication failures for username Az ssh-hoz autentikációnak jelszót kellett volna megadnom, de addig el se jutottam. Az alábbi egyszerű módon kezdeményeztem a kapcsolódást:\nssh username@\u0026lt;server\u0026gt; Megnéztem a -v és -vvv kapcsolók segítségével, hogy mi is történik, hol akad el a folyamat, de számomra semmi gyanúsat nem találtam.","title":"SSH returns 'Too Many Authentication Failures' error"},{"content":"A Diun - Docker Image Update Notifier, ahogy neve is sugallja, értesít bennünk arról, ha van újabb verziójú docker image.\nTelepítés docker image-gel A diun-t úgy állítottam be, hogy minden használatban lévő docker image-emet figyelje és 6 óránként ellenőrizze, hogy vannak-e újabb verziójúak. Értesítés küldésre egy saját script-et írtam, ami a Simplepush-on keresztül küld értesítést az okos telefonomra.\nAz docker-compose.yml fájl a diun service-szel:\nversion: \u0026#34;3.8\u0026#34; services: diun: image: crazymax/diun:latest container_name: diun restart: always command: serve volumes: - \u0026#34;./notif-script:/notif-script\u0026#34; - \u0026#34;./data:/data\u0026#34; - \u0026#34;./diun.yml:/diun.yml:ro\u0026#34; - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; environment: - \u0026#34;TZ=Europe/Budapest\u0026#34; - \u0026#34;LOG_LEVEL=info\u0026#34; - \u0026#34;LOG_JSON=false\u0026#34; - \u0026#34;SIMPLEPUSH_KEY=\u0026lt;your simplepush key\u0026gt;\u0026#34; notif-script könyvtár tartalmazza a notif.sh script-et, amely újabb docker image verzió esetén értesítést küld.\nA SIMPLEPUSH_KEY környezeti változóban adtam át a simplepush key-emet, amelyet a notif.sh script-ben használok fel, hogy magamnak címezzem a küldendő értesítést.\nA diun.yml fájl tartalmazza a diun beállításokat.\n# ./diun.yml watch: workers: 10 schedule: \u0026#34;0 */6 * * *\u0026#34; notif: script: cmd: \u0026#34;/notif-script/notif.sh\u0026#34; providers: docker: watchByDefault: true watchStopped: true # ./notif-script/notif.sh #!/bin/sh wget -q --post-data \\ \u0026#34;$(echo \u0026#34;key=$SIMPLEPUSH_KEY\u0026amp;title=New service update\u0026amp;msg=Docker tag $DIUN_ENTRY_IMAGE available.\u0026#34;)\u0026#34; \\ -O - \\ https://api.simplepush.io/send A notif.sh script-ben nem tudtam a curl parancsot használni, mert a diun docker image-ben az nem volt elérhető, de wget segítségével is lehet POST üzenetet küldeni.\n","permalink":"https://thomastrinn.github.io/blog/posts/diun/","summary":"A Diun - Docker Image Update Notifier, ahogy neve is sugallja, értesít bennünk arról, ha van újabb verziójú docker image.\nTelepítés docker image-gel A diun-t úgy állítottam be, hogy minden használatban lévő docker image-emet figyelje és 6 óránként ellenőrizze, hogy vannak-e újabb verziójúak. Értesítés küldésre egy saját script-et írtam, ami a Simplepush-on keresztül küld értesítést az okos telefonomra.\nAz docker-compose.yml fájl a diun service-szel:\nversion: \u0026#34;3.8\u0026#34; services: diun: image: crazymax/diun:latest container_name: diun restart: always command: serve volumes: - \u0026#34;.","title":"Docker Image Update Notifier"},{"content":"Hosszan kerestem egy olyan ingyenes szolgáltatást, ami egyszerű módon lehetővé teszi számomra, hogy push értesítéseket küldhessek okos telefonomra.\nA célom ezzel, hogy a home-lab server-em tudjon fontos eseményekről jelzést küldeni számomra.\nA szolgáltatás amit találtam a Simplepush. Ingyenesen limitált 100 push értesítés küldésre havonta, de ez nekem elegendő is.\nA szolgáltatás mellet ingyenesen android és ios alkalmazást is adnak.\nÉrtesítés küldése POST request-vel történik, így curl paranccsal is elvégezhető, részletek az api dokumentációjában találhatók.\n","permalink":"https://thomastrinn.github.io/blog/posts/simplepush/","summary":"Hosszan kerestem egy olyan ingyenes szolgáltatást, ami egyszerű módon lehetővé teszi számomra, hogy push értesítéseket küldhessek okos telefonomra.\nA célom ezzel, hogy a home-lab server-em tudjon fontos eseményekről jelzést küldeni számomra.\nA szolgáltatás amit találtam a Simplepush. Ingyenesen limitált 100 push értesítés küldésre havonta, de ez nekem elegendő is.\nA szolgáltatás mellet ingyenesen android és ios alkalmazást is adnak.\nÉrtesítés küldése POST request-vel történik, így curl paranccsal is elvégezhető, részletek az api dokumentációjában találhatók.","title":"Simplepush"},{"content":"A Samba segítségével fájl megosztást lehet lehetővé tenni Linux, Windows és MacOS operációs rendszerek számára.\nMeglepetésemre a telepítés és beállítás sokkal egyszerűbb volt, mint gondoltam.\nAz ubuntu oldalán található két leírás nagy segítséget adott:\nTutorial: Install and Configure Samba\nServer - Docs: File Server\nNincs más teendő, mint telepíteni a samba csomagot:\nsudo apt install samba A beállításokat pedig az /etc/samba/smb.conf fájlban tehetjük meg.\nA fájl alapértelmezett értékeit meghagytam, csupán a fájl végére felvettem a megosztásra szánt könyvtár beállításait:\n[data] comment = Data path = /data guest ok = no read only = no browsable = yes create mask = 0755 directory mask = 0755 [data]: a megosztás data néven lesz elérhető\ncomment: rövid leírás a megosztásról\npath: az útvonal a megosztani kívánt könyvtárhoz\nguest ok = no: jelszó nélkül nem érhető el a megosztás\nread only = no: írási jog engedélyezve a megosztásra\nbrowsable = yes: Windows kliensek számára engedélyezi a böngészést Windows Explorer-rel.\ncreate mask = 0755: az újonnan létrehozott fájlok jogosultságait határozza meg.\ndirectory mask = 0755: az újonnan létrehozott könyvtárak jogosultságait határozza meg.\nalapértelmezetten a mask értékek 0700, biztonsági okokból, a 0755-el group=rw jogosultság adható meg.\nA beállítások érvényesítése érdekében újra kell indítani a Samba service-t:\nsudo service smbd restart Ezt követően a felhasználóhoz rögzíteni kell egy Samba jelszót:\nsudo smbpasswd -a username A használt username-nek egy létező felhasználónak kell lennie.\nEzt követően már csatlakozhatunk a megosztott könyvtárhoz.\nLinux és Mac esetén a fájl kezelőben a Connect to Server lehetőség alatt az alábbit megadva:\nsmb://ip-address/data Windows esetén a File Manager-ben a fájl útvonal mezőben megadva:\n\\\\ip-address\\data ","permalink":"https://thomastrinn.github.io/blog/posts/set-up-file-server-with-samba/","summary":"A Samba segítségével fájl megosztást lehet lehetővé tenni Linux, Windows és MacOS operációs rendszerek számára.\nMeglepetésemre a telepítés és beállítás sokkal egyszerűbb volt, mint gondoltam.\nAz ubuntu oldalán található két leírás nagy segítséget adott:\nTutorial: Install and Configure Samba\nServer - Docs: File Server\nNincs más teendő, mint telepíteni a samba csomagot:\nsudo apt install samba A beállításokat pedig az /etc/samba/smb.conf fájlban tehetjük meg.\nA fájl alapértelmezett értékeit meghagytam, csupán a fájl végére felvettem a megosztásra szánt könyvtár beállításait:","title":"Set Up File Server With Samba"},{"content":"A home lab server operációs rendszerének némi kutakodás után az ubuntu server-t választottam, mivel linux szerverek esetén túlnyomórészt ezt használják.\nA kutakodás során az alábbiakat derítettem ki:\nA debian a legstabilabb és legbiztonságosabb, de hátránya hogy mindig elavult alkalmazásokat támogat, az előbbiek érdekében.\nAz ubuntu (debian alapú) a legnépszerűbb és legelterjedtebben használt, biztonságos és stabil, meghatározott release ciklussal és support-tal.\nAz arch linux nem igen kerül elő mint server operációs rendszer, de vannak vállalkozó szellemű cégek. Az előnye a rolling release model, aminek köszönhetően mindig a legfrissebb alkalmazás verziókat érjük el.\nA manjaro linux egy arch linux alapú rendszer, amely egy plusz tesztelési szakaszt követően enged be új alkalmazás verziókat, így stabilabb, mint az arch. Pár éve még létezett manjaro architect változat, ami DE nélkül manjaro verziót biztosított.\nAz alpine linux egy nagyon kicsi és kis erőforrás igényű linux.\nEzen információk birtokában figyelembe véve, hogy a telepítést követően egyből használhassam a server-t és bármi gond esetén könnyedén találjak segítséget a net-en, döntöttem az ubuntu mellett.\nUbuntu Server telepítése A telepítési útmutató átolvasását követően a telepítés nem volt bonyolult.\nTelepítés utáni beállítások 1. Private and Public Key Authentication A szerver ssh-val való eléréséhez felhasználó név és jelszó megadása helyett ssh-key használatát részesítem előnyben. Az ssh-key előállításáról és használatáról ezen korábbi bejegyzésemben található a leírás.\n2. Journal méret A journal idővel egyre több helyet foglal el, ennek határt szabhatunk ezen korábbi bejegyzésemben leírtak alapján.\n3. Auto update Debian alapú rendszerek esetén az unattended-upgrades csomag biztosít lehetőséget update-ek automatikus futtatására. Telepítését és beállítását ezen korábbi bejegyzésben taglalom.\n4. visudo editor beállítása A /etc/sudoers fájl szerkesztése a visudo paranccsal lehetséges, azonban alap esetben a szerkesztő ami használatban van az a nano, de ez módosítható.\nEgyszeri alkalommal az EDITOR környezeti változónak érték adással a parancs futtatásakor:\nsudo EDITOR=/usr/bin/vim visudo Így a vim szövegszerkesztővel szerkeszthetjük a fájlt. Azonban ez sem mindig működik, mert a sudoers fájlban be lehet állítva, hogy ne tartsa meg a környezeti fájlokat, ha a sudo használva van.\nAhhoz, hogy alapértelmezetten a vim legyen használva az /etc/sudoers fájlba, fel kell venni az alábbi sort:\nDefaults editor=/usr/bin/vim 5. shutdown jelszó nélkül A shutdown, poweroff és reboot parancsokhoz (vagy akár más parancsokhoz is) beállítható, hogy ne kelljen jelszót használni, de továbbra is csak sudo-val adhatóak ki. Ehhez a sudoers fájlban fel kell venni az alábbit:\n# Allow members of group sudo to execute shutdown, poweroff, reboot commands without password %sudo ALL=(ALL) NOPASSWD: /sbin/shutdown, /sbin/poweroff, /sbin/reboot ","permalink":"https://thomastrinn.github.io/blog/posts/home-lab-server-setup/","summary":"A home lab server operációs rendszerének némi kutakodás után az ubuntu server-t választottam, mivel linux szerverek esetén túlnyomórészt ezt használják.\nA kutakodás során az alábbiakat derítettem ki:\nA debian a legstabilabb és legbiztonságosabb, de hátránya hogy mindig elavult alkalmazásokat támogat, az előbbiek érdekében.\nAz ubuntu (debian alapú) a legnépszerűbb és legelterjedtebben használt, biztonságos és stabil, meghatározott release ciklussal és support-tal.\nAz arch linux nem igen kerül elő mint server operációs rendszer, de vannak vállalkozó szellemű cégek.","title":"Home Lab Server Setup"},{"content":"Hosszas fontolgatás után úgy döntöttem eljött az ideje a Raspberry Pi mini számítógép kalandozásaim után, hogy egy erősebb hardware-t keressek, amit Home Lab Server-nek használhatok.\nElvárások:\nKicsi, kompakt számítógép ház\nCsendes\nM.2 NVMe SSD támogatás\n2,5\u0026rsquo;\u0026rsquo;\u0026rsquo; HDD elférjen a számítógép házban\nTöbb magos CPU\nJó teljesítmény, alacsony fogyasztás\nEgy hosszas kutakodást követően a Minisforum UM340 Mini PC-re esett a választásom.\nNagy előnye, hogy barebone változatban is elérhető volt, így a memóriát, SSD-t és HDD-t magam választhattam meg.\nA CPU teljesítményéhez képest alacsony fogyasztású a konkurens hasonló teljesítményű CPU-khoz képest.\nProcessor AMD Ryzen™ 5 3450U, 4 Cores/8 Threads, DDR4 Dual channel (2400Mhz), PCIe® 3.0, cTDP: 12-35W, Q2 2020 Memory G.Skill Ripjaws DDR4 SO-DIM, 2x8GB SSD Samsung 970 EVO Plus 256 GB HDD Western Digital Blue 2 TB A ház kialakítása nagyon jó, könnyen nyitható és az alkatrészek könnyedén beszerelhetőek voltak.\nEgy negatívuma van, hogy a ventilátor zajosabb, mint amire számítottam.\n","permalink":"https://thomastrinn.github.io/blog/posts/first-mini-home-lab-server-metal/","summary":"Hosszas fontolgatás után úgy döntöttem eljött az ideje a Raspberry Pi mini számítógép kalandozásaim után, hogy egy erősebb hardware-t keressek, amit Home Lab Server-nek használhatok.\nElvárások:\nKicsi, kompakt számítógép ház\nCsendes\nM.2 NVMe SSD támogatás\n2,5\u0026rsquo;\u0026rsquo;\u0026rsquo; HDD elférjen a számítógép házban\nTöbb magos CPU\nJó teljesítmény, alacsony fogyasztás\nEgy hosszas kutakodást követően a Minisforum UM340 Mini PC-re esett a választásom.\nNagy előnye, hogy barebone változatban is elérhető volt, így a memóriát, SSD-t és HDD-t magam választhattam meg.","title":"First Mini Home Lab Server - The Metal"},{"content":" The notion of a “magic wormhole” comes from the image of two distant wizards speaking the same enchanted phrase at the same time, and causing a mystical connection to pop into existence between them. The wizards then throw books into the wormhole and they fall out the other side. Transferring files securely should be that easy. \u0026ndash; magic-worhole official website\nA wormhole lehetővé teszi tetszőleges méretű fájlok és könyvtárak (vagy rövid szövegrészek) átjutását egyik számítógépről a másikra. A két végpont azonosítása azonos „féreglyuk-kódok” használatával történik: a küldő gép generálja és megjeleníti a kódot, amelyet ezután be kell gépelni a fogadó gépen.\nHasználat Küldő:\n$ wormhole send hello.txt Sending 82 Bytes file named \u0026#39;hello.txt\u0026#39; Wormhole code is: 9-guitarist-geiger On the other computer, please run: wormhole receive 9-guitarist-geiger Fogadó:\n$ wormhole receive 9-guitarist-geiger Receiving file (82 Bytes) into: hello.txt ok? (Y/n): y Receiving (-\u0026gt;tcp:10.0.1.32:43262).. 100%|███████| 82.0/82.0 [00:00\u0026lt;00:00, 618B/s] Received file written to hello.txt GUI alkalmazás Cross platform gui alkalmazásban is elérhető: wormhole-gui\n","permalink":"https://thomastrinn.github.io/blog/posts/magic-wormhole-secure-file-transfer/","summary":"The notion of a “magic wormhole” comes from the image of two distant wizards speaking the same enchanted phrase at the same time, and causing a mystical connection to pop into existence between them. The wizards then throw books into the wormhole and they fall out the other side. Transferring files securely should be that easy. \u0026ndash; magic-worhole official website\nA wormhole lehetővé teszi tetszőleges méretű fájlok és könyvtárak (vagy rövid szövegrészek) átjutását egyik számítógépről a másikra.","title":"Magic Wormhole: Secure File Transfer"},{"content":"A Ventoy egy open source eszköz, amely segítségével boot-olható USB drive-ot készíthetünk ISO/WIM/IMG/VHD(x)/EFI fájlok számára.\nVentoy használata Töltsük le vagy telepítsük a Ventoy programot. A ventoygui alkalmazás indítása után egy egyszerű felülettel találkozunk: kiválaszthatjuk az eszközt, amelyre fel kívánjuk telepíteni megadhatjuk, hogy MBR vagy GPT partíciós stílust akarunk-e használni megadhatjuk, a ventoy partíció méretét. A beállítások után telepíthetjük a ventoy-t, vagy ha már korábban telepítettük, akkor frissíthetjük. A telepítést követően a Ventoy partícióra felmásolhatjuk a .iso fájlokat és már használhatjuk is. Összegzés A Ventoy használata egyszerű és általa egyszerűvé válik boot-olható USB drive létrehozása. Amikor egy új OS iso-t használni akarunk, azt csak fel kell másolni a Ventoy partícióra és készen is vagyunk. Nincs szükség formázásra és az új iso flash-elésére. Sőt egyszerre több .iso-t is felmásolhatunk és így egy multi-iso boot flash drive-ot készíthetünk.\n","permalink":"https://thomastrinn.github.io/blog/posts/create-a-multi-iso-bootable-flash-drive/","summary":"A Ventoy egy open source eszköz, amely segítségével boot-olható USB drive-ot készíthetünk ISO/WIM/IMG/VHD(x)/EFI fájlok számára.\nVentoy használata Töltsük le vagy telepítsük a Ventoy programot. A ventoygui alkalmazás indítása után egy egyszerű felülettel találkozunk: kiválaszthatjuk az eszközt, amelyre fel kívánjuk telepíteni megadhatjuk, hogy MBR vagy GPT partíciós stílust akarunk-e használni megadhatjuk, a ventoy partíció méretét. A beállítások után telepíthetjük a ventoy-t, vagy ha már korábban telepítettük, akkor frissíthetjük. A telepítést követően a Ventoy partícióra felmásolhatjuk a .","title":"Create a Multi-ISO Bootable Flash Drive"},{"content":"Olyan open source szoftvereket gyűjtöttem össze, amelyek fájl titkosítást tesznek lehetővé, illetve cloud szolgáltatás esetén end-to-end encryption-t támogatnak.\n7-Zip A 7-Zip egy fájl arhiváló, amely jelszóval védett archív fájl létrehozását is lehetővé teszi.\nHat.sh A Hat.sh egy webes alkalmazás, amely a böngészőben fájl titkosítást tesz lehetővé.\nLUKS - Linux Unified Key Setup A LUKS segítségével titkosított partíciót hozhatunk létre.\nVeraCrypt A VeraCrypt egy disk encryption alkalmazás, amely képes meglevő partíciót titkosítani, illetve virtuális titkosított meghajtót létrehozni.\nCryptomator A cryptomator fő célja, hogy az általunk használt cloud szolgáltatásokat titkosítsa, ezt úgy teszi lehetővé, hogy jelszóval védett könyvtár - úgynevezett vault - létrehozását teszi lehetővé, amelyben tárolhatjuk fájljainkat.\nFilen A Filen egy úgynevezett zero knowledge end-to-end encrypted cloud storage.\nIcedrive Az icedrive is egy zero knowledge end-to-end encrypted cloud storage.\nMega A Mega is egy zero knowledge end-to-end encrypted cloud storage.\nCryptPad A CryptPad end-to-end encryption-t alkalmazó collaboration suite.\nNextCloud A NextCloud egy self-host cloud szolgáltatás.\nOwnCloud Az OwnCloud egy self-host cloud storage.\nSeafile A Seafile egy self-host cloud storage.\nPiwigo A Piwigo egy self-host fénykép kezelő szolgáltatás.\nStorj A Storj decentralizált hálózaton biztosít end-to-end encrypted cloud storage-et.\nSia A Sia egy decentralizált cloud storage.\n","permalink":"https://thomastrinn.github.io/blog/posts/encrypted-data-storage/","summary":"Olyan open source szoftvereket gyűjtöttem össze, amelyek fájl titkosítást tesznek lehetővé, illetve cloud szolgáltatás esetén end-to-end encryption-t támogatnak.\n7-Zip A 7-Zip egy fájl arhiváló, amely jelszóval védett archív fájl létrehozását is lehetővé teszi.\nHat.sh A Hat.sh egy webes alkalmazás, amely a böngészőben fájl titkosítást tesz lehetővé.\nLUKS - Linux Unified Key Setup A LUKS segítségével titkosított partíciót hozhatunk létre.\nVeraCrypt A VeraCrypt egy disk encryption alkalmazás, amely képes meglevő partíciót titkosítani, illetve virtuális titkosított meghajtót létrehozni.","title":"Encrypted Data Storage"},{"content":"Jelen bejegyzésben megmutatom, hogyan hoztam létre ezt a blog oldalt.\nÁttekintés Az oldalt a Hugo nevű statikus oldal generálóval készítem, publikusan elérhetővé pedig a GitHub Pages segítségével teszem.\nGitHub-on két repository-t készítettem, az egyik tartalmazza az oldal forráskódját és a tartalmakat, a másik pedig a generált oldalt.\nBlog létrehozása Hugo-val Hugo telepítése A telepítés igen egyszerű, a hivatalos útmutató alapján bármilyen operációs rendszerre fel tudjuk telepíteni.\nBlog oldal létrehozása A blog oldal létrehozása a hugo programmal:\nhugo new site blog amely az adott könyvtárban kiadva létrehoz egy blog könyvtárat. Ezen könyvtárat inicializáltam mint git repository:\ncd blog git init Theme választása Az oldal létrehozás után egy theme-át kell választani és beállítani. A Hugo Themes oldalon nagy választék van, innen a PaperMod theme-át választottam.\nTheme beállítása A repository-hoz hozzáadtam submodule-ként a PaperMod theme-át:\ngit submodule add \\ https://github.com/adityatelange/hugo-PaperMod.git \\ themes/PaperMod \\ --depth=1 vagy\ngit submodule--helper add \\ https://github.com/adityatelange/hugo-PaperMod.git \\ themes/PaperMod \\ --depth=1 Majd a config.toml fájlba megadtam, hogy ez legyen az oldal által használt theme:\necho theme = \\\u0026#34;PaperMod\\\u0026#34; \u0026gt;\u0026gt; config.toml A beállítások eredményességéről meggyőződhetünk a hugo server indításával:\nhugo server -D A -D kapcsoló jelentése, hogy draft módban indítjuk.\nAz oldalt a http://localhost:1313/blog-on meg tudjuk tekinteni.\nBlog Page létrehozása GitHub-on Blog forráskód repository GitHub-on létrehoztam egy blog-hugo nevű repository-t és beállítottam mint remote repository.\nLétrehoztam egy .gitignore fájlt az alábbi tartalommal:\n.hugo_build.lock # Generated files by hugo /public/ /resources/_gen/ Majd feltettem github-ra:\ngit add . git commit -m \u0026#34;init blog\u0026#34; git push -u origin master Blog statikus oldal repository A Hugo által generált oldalt egy külön repository-ban tárolom.\nA Hugo a generált oldalt a public könyvtárba hozza létre és ezen repository ezt fogja tartalmazni.\nGitHub-on létrehoztam egy blog nevű repository-t és submodule-ként felvettem a public könyvtárhoz:\ngit submodule--helper add git@github.com:\u0026lt;username\u0026gt;/blog.git public A fő repository-ban változásként látni fogjuk a .gitmodules-t és a public könyvtárat. Adjuk hozzá a verziókövetőhöz:\ngit add . git commit -m \u0026#34;add public submodule\u0026#34; git push GitHub Pages beállítása A GitHub-on a blog repository Settings-ei közt a Pages menüben Source-ként válasszuk ki a brach-ünket és folder-ként a / (root)-ot.\nEzt követően a fő repository-ban levő Hugo config fájlt módosítanunk kell, hogy a GitHub Page-ünk url-jét tartalmazza.\nA GitHub Page-ünk url-je két részből tevődik össze:\naz első rész: a github user nevünk, amit a .github.io követ a második rész: a repository neve, ami jelen esetben blog Ezen felül az oldalunk címét is meg kell adni, lecserélve a generált címet.\nEzen beállításokat a config.toml fájlban tehetjük meg:\nbaseURL = \u0026#39;https://\u0026lt;user-name\u0026gt;.github.io/blog/\u0026#39; languageCode = \u0026#39;en-us\u0026#39; title = \u0026#39;Blog\u0026#39; theme = \u0026#34;PaperMod\u0026#34; Ezzel előkészítettük a terepet az oldal generáláshoz és publikáláshoz.\nMódosításainkat töltsük fel:\ngit add . git commit -m \u0026#34;update config\u0026#34; git push Hugo statikus oldal generálás A blog oldal legenerálása:\nhugo Ezt követően a public könyvtárba lépve a generált oldalt fel tudjuk tölteni:\ncd public git add . git commit -m \u0026#34;init blog\u0026#34; git push A feltöltést követően nemsokára a GitHub Page deploy lefut és a Blog oldal elérhetővé válik.\n","permalink":"https://thomastrinn.github.io/blog/posts/create-a-blog-with-hugo-and-hosting-on-github-pages/","summary":"Jelen bejegyzésben megmutatom, hogyan hoztam létre ezt a blog oldalt.\nÁttekintés Az oldalt a Hugo nevű statikus oldal generálóval készítem, publikusan elérhetővé pedig a GitHub Pages segítségével teszem.\nGitHub-on két repository-t készítettem, az egyik tartalmazza az oldal forráskódját és a tartalmakat, a másik pedig a generált oldalt.\nBlog létrehozása Hugo-val Hugo telepítése A telepítés igen egyszerű, a hivatalos útmutató alapján bármilyen operációs rendszerre fel tudjuk telepíteni.\nBlog oldal létrehozása A blog oldal létrehozása a hugo programmal:","title":"Create a Blog With Hugo and Hosting on GitHub Pages"},{"content":"A Raspberry Pi 3B+ 1 GiB memóriája előfordul, hogy kevésnek bizonyul, illetve a swap használata az SD kártyán lassúvá teheti a rendszerünket. Ennek orvoslásaként használható a zram.\nMi is az a zram? zram, formerly called compcache, is a Linux kernel module for creating a compressed block device in RAM, i.e. a RAM disk with on-the-fly disk compression. The block device created with zram can then be used for swap or as general-purpose RAM disk. The two most common uses for zram are for the storage of temporary files (/tmp) and as a swap device. Initially, zram had only the latter function, hence the original name \u0026ldquo;compcache\u0026rdquo; (\u0026ldquo;compressed cache\u0026rdquo;). \u0026ndash; Wikipedia\nMagyarra fordítva a zram: a rendelkezésre álló memóriánk egy részét arra használjuk, hogy benne tömörített módon tároljunk adatokat. Ezt a részt pedig swap területként használjuk fel. Így lényegében a rendelkezésre álló memóriát növelhetjük.\nHatása a teljesítményre A tömörítés és kicsomagolás természetesen igénybe veszi a CPU-t, de ez már egy kiforrott technológia és a Raspberry Pi 3B+ 4 magos CPU-jára csak minimális terhelést jelent.\nzram-swap-config telepítése A zram-swap-config package segítségével könnyen telepíthető és beállítható a zram swap használatra:\n# ensure that git is installed apt-get install git # clone the zram-swap-config repository git clone https://github.com/StuartIanNaylor/zram-swap-config cd zram-swap-config # install zram-swap-config chmod +x install.sh \u0026amp;\u0026amp; sudo ./install.sh # cleanup installation files cd .. rm -r zram-swap-config zram-swap-config beállítása A beállítások a /etc/zram-swap-config.conf fájlban találhatóak, a lehetőségek jól dokumentáltak.\nAlapvetően három fő beállítási lehetőségünk van:\nMEM_FACTOR: százalékban megadva a használni kívánt memóriát, pl.: ha értéke 25, akkor a memória 25%-a lesz zram-nak felhasználva. DRIVE_FACTOR: a tömörítési arány, pl.: ha értéke 200, akkor az 2.00-ás tömörítési arányt jelent. COMP_ALG: a használni kívánt tömörítési algoritmus. Az elérhető tömörítési algoritmusokat az alábbi módon tekinthetjük meg:\ncat /sys/block/zram0/comp_algorithm A zram-swap-config repository-ban egy táblázatban megtekinthetjük az egyes tömörítési algoritmusok tulajdonságait.\nMagamnak az alábbi értékeket választottam:\nMEM_FACTOR=50 DRIVE_FACTOR=300 COMP_ALG=lz4 Ezen értékek értelmében 50% x 1 GiB x 3.00 = 1,5 GiB swap-ot eredményez.\nA telepítést és beállítást követően újra kell indítani a Raspberry Pi-t, hogy azok érvényesüljenek.\nRaspbian file swap service kikapcsolása A swap fájl használata alapvetően az SD kártya I/O teljesítménye miatt nagyban lassíthatja a rendszert (sőt a sok írás miatt az SD kártya élettartamát is csökkenti), így érdemes kikapcsolni.\nA swap kikapcsolása:\nsudo systemctl disable dphys-swapfile Vagy ha törölni szeretnénk:\nsudo apt-get remove dphys-swapfile Összegzés A zram segítségével ki tudjuk terjeszteni a Raspberry Pi 3B+ 1 GiB memóriáját 2 GiB-ra, némi CPU erőforrás árán.\n","permalink":"https://thomastrinn.github.io/blog/posts/using-zram-with-raspberry-pi/","summary":"A Raspberry Pi 3B+ 1 GiB memóriája előfordul, hogy kevésnek bizonyul, illetve a swap használata az SD kártyán lassúvá teheti a rendszerünket. Ennek orvoslásaként használható a zram.\nMi is az a zram? zram, formerly called compcache, is a Linux kernel module for creating a compressed block device in RAM, i.e. a RAM disk with on-the-fly disk compression. The block device created with zram can then be used for swap or as general-purpose RAM disk.","title":"Using Zram With Raspberry Pi"},{"content":"Szerverre ssh-val történő belépéshez felhasználónév és jelszó megadás helyett használhatunk ssh kulcsot is.\nSSH kulcsot generálni az ssh-keygen programmal tudunk:\nssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; -f ~/.ssh/some-name A fenti paranccsal 4096 bit-es RSA kulcsot hozunk létre, a -C kapcsolóval megjegyzést fűzünk a kulcshoz a könnyebb beazonosítás végett, ami lehet az e-mail címünk, de nem kötelező, továbbá az -f kapcsolóval megadjuk a kulcs nevét (nem kötelező megadni, ha nem adjuk meg, akkor generálás közben kéri és van alapértelmezett értéke). Fontos, hogy a kulcs a felhasználó home könyvtárában levő .ssh könyvtárba kerüljön.\nA generálás során passphrase-t is megadhatunk, de nem kötelező. Ha megadunk, akkor a kulcsot ezen passphrase-el feloldva tudjuk használni.\nA generálás végeztével a felhasználó .ssh könyvtárába két fájl fog megjelenni: az egyik a megadott névvel, ez lesz a private key, a másik a megadott név .pub kiterjesztéssel, ami a public key.\nA public key-t a szerverre az ssh-copy-id programmal tudjuk feltenni:\nssh-copy-id -i ~/.ssh/some-name.pub username@remote-server A -i kapcsolóval megadjuk a public key elérését, amit a szerverre akarunk másolni.\nEzt követően úgy léphetünk be a szerverre, hogy nem kell megadnunk a jelszavunkat, az authentikáció megtörténik a public és a private key segítségével.\nTovábbi részletekért az arch wiki - ssh keys bejegyzést tudom ajánlani.\n","permalink":"https://thomastrinn.github.io/blog/posts/private-and-public-key-authentication/","summary":"Szerverre ssh-val történő belépéshez felhasználónév és jelszó megadás helyett használhatunk ssh kulcsot is.\nSSH kulcsot generálni az ssh-keygen programmal tudunk:\nssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; -f ~/.ssh/some-name A fenti paranccsal 4096 bit-es RSA kulcsot hozunk létre, a -C kapcsolóval megjegyzést fűzünk a kulcshoz a könnyebb beazonosítás végett, ami lehet az e-mail címünk, de nem kötelező, továbbá az -f kapcsolóval megadjuk a kulcs nevét (nem kötelező megadni, ha nem adjuk meg, akkor generálás közben kéri és van alapértelmezett értéke).","title":"Private and Public Key Authentication"},{"content":"Riasztások segítségével értesülhetünk arról, ha valami probléma lépett fel a rendszerünkben.\nA Grafana lehetőséget biztosít riasztások létrehozására. A hivatalos dokumentáció segítségével könnyedén létre tudtam hozni riasztásokat. Úgynevezett alert rule-t kell definiálni, ami áll egy query-ből, ami meghatározza azt az értéket, amit figyelni kívánunk és áll egy condition-ből, amiben a feltételt határozhatjuk meg a riasztáshoz.\nAhhoz, hogy a riasztásról értesülhessek a Gotify message server-t használtam. Ez biztosít egy web-es felületet a message-ek megjelenítésére, illetve van android alkalmazás is hozzá.\nA Gotify server-t docker container-ként futtatom az alábbi docker-compose.yml fájlban megadott módon:\nversion: \u0026#34;3\u0026#34; services: gotify: image: gotify/server-arm7:latest container_name: gotify ports: - 8020:80 environment: - GOTIFY_DEFAULTUSER_PASS=admin - TZ=\u0026#39;Europe/Budapest\u0026#39; volumes: - gotify_data:/app/data restart: unless-stopped volumes: gotify_data: driver: local Grafana-ban riasztás esetén értesítés küldéshez contact point-ot kell létrehozni. Itt webhook-ként vettem fel a Gotify servert.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-10-server-monitoring-part-06-grafana-alerts/","summary":"Riasztások segítségével értesülhetünk arról, ha valami probléma lépett fel a rendszerünkben.\nA Grafana lehetőséget biztosít riasztások létrehozására. A hivatalos dokumentáció segítségével könnyedén létre tudtam hozni riasztásokat. Úgynevezett alert rule-t kell definiálni, ami áll egy query-ből, ami meghatározza azt az értéket, amit figyelni kívánunk és áll egy condition-ből, amiben a feltételt határozhatjuk meg a riasztáshoz.\nAhhoz, hogy a riasztásról értesülhessek a Gotify message server-t használtam. Ez biztosít egy web-es felületet a message-ek megjelenítésére, illetve van android alkalmazás is hozzá.","title":"Raspberry Pi 3 - Part 10 Server Monitoring Part 6 - Grafana Alerts"},{"content":"A Server Monitoring sorozat folytatásaként most a docker container logok gyűjtésével és megjelenítésével foglalkozom.\nDocker container logok gyűjtése A container logok helye: /var/lib/docker/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log\nAhol a \u0026lt;container-id\u0026gt; a docker container-ek egyedi azonosítója.\nA docker container logok gyűjtéséhez a Promtail service számára a /var/lib/docker könyvtárat kell volume-ként becsatolni:\npromtail: image: grafana/promtail:latest container_name: promtail command: -config.file=/etc/promtail/config.yml volumes: - ./promtail/promtail-config.yml:/etc/promtail/config.yml - /var/log:/var/log:ro - /var/lib/docker/:/var/lib/docker:ro restart: unless-stopped Így a promtail container eléri a host docker log-jait és most már be tudjuk állítani, hogy a promtail a /var/lib/docker/containers/*/*log fájlokat figyelve a logokat elküldje a Loki-nak. Ehhez a promtail/promtail-config.yml fájlba a scrape_configs alá egy új job-ot kell felvenni:\n- job_name: containers static_configs: - targets: - localhost labels: job: containerlogs host: raspberrypi __path__: /var/lib/docker/containers/*/*log Docker log kimenet kiegészítése Ha csak a fenti beállításokat használnánk, akkor Grafana-ban csak a \u0026lt;container-id\u0026gt;-json.log fájlnevek állnának rendelkezésünkre, amik nem teszik egyszerűvé egy kívánt container log-jának megtekintését, mert ahhoz kellene tudnunk, hogy a vizsgálni kívánt container-nek mi az azonosítója.\nA json-file log driver esetében van lehetőségünk módosítani a log kimenetet. A log-opts paraméterben adható meg a log tag-je, ami azonosítja a container log üzenetét. Alapesetben a container id-ja lesz felhasználva, amit felülbírálhatunk.\nA tag értékének az alábbit választottam:\n{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}} Azaz tartalmazza a docker image nevét, a container nevét, a docker image teljes id-ját és a container teljes id-ját.\nKét lehetőségünk van a tag beállítására: adott container indításánál vagy globálisan, így minden újjonnan létrehozott container esetében érvényesül az érték.\nGlobálisan a /etc/docker/daemon.json fájlban adható meg:\n{ \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;tag\u0026#34;: \u0026#34;{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}\u0026#34; } } A beállítás érvényesítéséhez újra kell indítani a docker-t:\nsudo systemctl restart docker Illetve a módosítás csak új container-ek esetében kerül alkalmazásra.\nPromtail pipeline Most, hogy a container logok kiegészültek a container-ek nevével is, promtail-ben ezen extra adatokat ki kell nyernünk, hogy a Loki felé mint speciális mezők küldje el. Erre használható a Promtail Pipeline.\nEnnek felhasználásával a promtail/promtail-config.yml fájlban felvett containers job kiegészítve egy pipeline-al, ami kiveszi a container logokból az image nevet, a container nevet, az image id-t és a container id-t, az alábbi módon néz ki:\n- job_name: system static_configs: - targets: - localhost labels: job: varlogs host: raspberrypi __path__: /var/log/*log - job_name: containers static_configs: - targets: - localhost labels: job: containerlogs host: raspberrypi __path__: /var/lib/docker/containers/*/*log # --log-opt tag=\u0026#34;{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}\u0026#34; pipeline_stages: - json: expressions: stream: stream attrs: attrs tag: attrs.tag - regex: expression: (?P\u0026lt;image_name\u0026gt;(?:[^|]*[^|])).(?P\u0026lt;container_name\u0026gt;(?:[^|]*[^|])).(?P\u0026lt;image_id\u0026gt;(?:[^|]*[^|])).(?P\u0026lt;container_id\u0026gt;(?:[^|]*[^|])) source: tag - labels: tag: stream: image_name: container_name: image_id: container_id A beállítások érvényesítéséhez a futó promtail container-t le kell állítanunk, majd törölnünk és újra létrehoznunk.\nContainer logok megjelenítése Most, hogy az új container log kimenetet megadtuk, promtail-t beállítottuk, hogy a log sorokból kinyerje az adott container nevét, Grafana-ban lehetőségünk van egy-egy container logjának megtekintésére.\nAz Explore menüben Loki-t mint datasource-t kiválasztva a Log browser-en belül a label-ek közt megtaláljuk az alábbiakat is:\ncontainer_id container_name image_id image_name Ezen új label-ek azok, amelyeket promtail tett elérhetővé.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-09-server-monitoring-part-05-container-logs/","summary":"A Server Monitoring sorozat folytatásaként most a docker container logok gyűjtésével és megjelenítésével foglalkozom.\nDocker container logok gyűjtése A container logok helye: /var/lib/docker/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log\nAhol a \u0026lt;container-id\u0026gt; a docker container-ek egyedi azonosítója.\nA docker container logok gyűjtéséhez a Promtail service számára a /var/lib/docker könyvtárat kell volume-ként becsatolni:\npromtail: image: grafana/promtail:latest container_name: promtail command: -config.file=/etc/promtail/config.yml volumes: - ./promtail/promtail-config.yml:/etc/promtail/config.yml - /var/log:/var/log:ro - /var/lib/docker/:/var/lib/docker:ro restart: unless-stopped Így a promtail container eléri a host docker log-jait és most már be tudjuk állítani, hogy a promtail a /var/lib/docker/containers/*/*log fájlokat figyelve a logokat elküldje a Loki-nak.","title":"Raspberry Pi 3 - Part 9: Server Monitoring Part 5 - Container Logs"},{"content":"A Server Monitoring sorozat folytatásaként most a linux host system log gyűjtésével és megjelenítésével foglalkozom.\nSystem log gyűjtése A system log gyűjtéséhez a Promtail service számára a linux host /var/log könyvtárát kell volume-ként becsatolni. Az eredeti docker-compose promtail service ezzel kiegészítve az alábbi:\npromtail: image: grafana/promtail:latest container_name: promtail command: -config.file=/etc/promtail/config.yml volumes: - ./promtail/promtail-config.yml:/etc/promtail/config.yml - /var/log:/var/log:ro restart: unless-stopped Így a promtail container eléri a host system log-jait és most már be tudjuk állítani, hogy a promtail a /var/log könyvtárat figyelve a logokat elküldje a Loki-nak. Ehhez a promtail/promtail-config.yml fájlba a scrape_configs alá egy új job-ot kell felvennünk:\n- job_name: system static_configs: - targets: - localhost labels: job: varlogs host: raspberrypi __path__: /var/log/*log A beállítások érvényesítéséhez a futó promtail container-t le kell állítanunk, majd törölnünk és újra létrehoznunk.\nSystem log megjelenítése Grafana-ban az Explore menüben Loki-t mint datasource-t kiválasztva van lehetőségünk a logok megtekintésére. Itt forrásként megadhatjuk a varlogs nevű job-ot, vagy a raspberrypi nevű host-ot és így egyszerre megtekinthető az összes log, vagy lehetőségünk van egy logot a fájl név alapján megtekinteni.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-08-server-monitoring-part-04-system-logs/","summary":"A Server Monitoring sorozat folytatásaként most a linux host system log gyűjtésével és megjelenítésével foglalkozom.\nSystem log gyűjtése A system log gyűjtéséhez a Promtail service számára a linux host /var/log könyvtárát kell volume-ként becsatolni. Az eredeti docker-compose promtail service ezzel kiegészítve az alábbi:\npromtail: image: grafana/promtail:latest container_name: promtail command: -config.file=/etc/promtail/config.yml volumes: - ./promtail/promtail-config.yml:/etc/promtail/config.yml - /var/log:/var/log:ro restart: unless-stopped Így a promtail container eléri a host system log-jait és most már be tudjuk állítani, hogy a promtail a /var/log könyvtárat figyelve a logokat elküldje a Loki-nak.","title":"Raspberry Pi 3 - Part 8: Server Monitoring Part 4 - System Logs"},{"content":"A Server Monitoring sorozat folytatásaként most a futó docker containerek erőforrás igényeit szeretném kinyerni és megjeleníteni.\nDocker daemon Van lehetőség közvetlenül a docker daemon-ból kinyerni metric adatokat, a hivatalos docker dokumentációban találtam leírást hozzá. Jelenleg még fejlesztés alatt van, így csak experimental módban lehet használni.\nA /etc/docker/daemon.json fájlban adható meg a metric végpont engedélyezése:\n{ \u0026#34;metrics-addr\u0026#34; : \u0026#34;127.0.0.1:9323\u0026#34;, \u0026#34;experimental\u0026#34; : true } Ahhoz, hogy érvényesüljön a beállítás újra kell indítani a docker daemon-t:\nsudo systemctl restart docker.service Ezt követően a docker daemon a 9323-as porton Prometheus kompatibilis metric adatokat szolgáltat, ezt a következőképp ellenőrizhetjük:\nPort ellenőrzése:\nsudo lsof -i -P -n | grep LISTEN | grep 9323 Metric végpont ellenőrzése:\ncurl localhost:9323/metrics Előre elkészített Grafana dashboard-ot nem találtam a docker daemon-hoz, illetve a docker daemon által szolgáltatott metric végponton a docker daemon internal állapotáról kapunk metric adatokat és a futó docker container-ekről, azok erőforrás igényéről nem. Így ezt nem tudom felhasználni.\ncAdvisor A cAdvisor egy igen népszerű eszköz, azonban sajnos arm architektúrára nincs docker image-ük, de vannak nem hivatalos arm buildek, ezek közül a zcube/cadvisor-t választottam.\nA cAdvisor docker-compose service-ként az alábbi módon hoztam létre:\ncadvisor: image: zcube/cadvisor:latest container_name: cadvisor ports: - \u0026#34;8080:8080\u0026#34; volumes: - /:/rootfs:ro - /var/run:/var/run:ro - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro - /dev/disk/:/dev/disk:ro devices: - /dev/kmsg privileged: true ipc: shareable security_opt: - label=disable restart: unless-stopped amely a grafana-stack docker-compose.yml fájlba vettem fel, hogy egy docker network-ön legyen a stack többi részével.\ncAdvisor metric adatok gyűjtése Ahhoz, hogy Prometheus elérje a cAdvicser metric végpontját azt a prometheus.yml fájlban fel kell venni a scrape_configs rész alá:\nglobal: scrape_interval: 5s scrape_configs: - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] A módosítást, kiegészítést követően újra kell olvastatni Prometheus-al a config fájlt.\ncAdvisor metric adatok megjelenítése A cAdvisor által szolgáltatott metric adatok, most már Prometheus-ba kerülnek, így egy grafana dashboard-ot is választottam: Docker and OS Metrics for Raspberry Pi by oijkn\nEz egyben a node exporter adatokból is dolgozik, nemcsak a cAdvisor adataiból, így ezt igen praktikusnak találom.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-07-server-monitoring-part-03-container-metrics/","summary":"A Server Monitoring sorozat folytatásaként most a futó docker containerek erőforrás igényeit szeretném kinyerni és megjeleníteni.\nDocker daemon Van lehetőség közvetlenül a docker daemon-ból kinyerni metric adatokat, a hivatalos docker dokumentációban találtam leírást hozzá. Jelenleg még fejlesztés alatt van, így csak experimental módban lehet használni.\nA /etc/docker/daemon.json fájlban adható meg a metric végpont engedélyezése:\n{ \u0026#34;metrics-addr\u0026#34; : \u0026#34;127.0.0.1:9323\u0026#34;, \u0026#34;experimental\u0026#34; : true } Ahhoz, hogy érvényesüljön a beállítás újra kell indítani a docker daemon-t:","title":"Raspberry Pi 3 - Part 7: Server Monitoring Part 3 - Container Metrics"},{"content":"A Server Monitoring sorozat folytatásaként most a hardware és os metric adatok gyűjtésével és megjelenítésével foglalkozom.\nSystem metric adatok elérése A Prometheus egyik hivatalos metric exporter-je a Node exporter , amely pont megfelel céljaim számára.\nA node exporter futtatható docker container-ként és docker-compose service-ként az alábbi módon hoztam létre:\nnode_exporter: image: quay.io/prometheus/node-exporter:latest container_name: node_exporter command: --path.rootfs=/host pid: host ports: - \u0026#34;9100:9100\u0026#34; volumes: - /:/host:ro,rslave restart: unless-stopped amelyet a grafana-stack docker-compose.yml fájlba vettem fel, hogy egy docker network-on legyen a stack többi részével.\nSystem metric adatok gyűjtése Annak érdekében, hogy Prometheus tudjon a node exporter által publikált /metrics végponton szolgáltatott metric adatokat gyűjteni, úgy azt fel kell venni a prometheus.yml fájlban a scrape_configs-ok alá:\nglobal: scrape_interval: 5s scrape_configs: - job_name: \u0026#39;node_exporter\u0026#39; static_configs: - targets: [\u0026#39;node_exporter:9100\u0026#39;] Annak érdekben, hogy érvényesüljenek a módosításaink Prometheus-nak újra kell olvasnia a config fájlt. Ez elérhető a container újraindításával, vagy ha aktiváltuk a futás idejű config fájl újraolvasását, úgy a serveren futtassuk az alábbi parancsot:\ncurl -X POST http://localhost:9090/-/reload Ezt követően a Prometheus web ui-ban a Targets alatt látnunk kell az imént felvett node_exporter nevű job-ot.\nSystem metric adatok megjelenítése Miután a node exporter által szolgáltatott metric adatok Prometheus-ban tárolásra kerülnek, úgy a Grafana-ban lehetőségünk van ezen metric adatokat megjeleníteni.\nNagyon sok előre elkészített dashboard létezik a node exporter-hez, az én választásom a Node Exporter Full by rfraile-ra esett.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-06-server-monitoring-part-02-system-metrics/","summary":"A Server Monitoring sorozat folytatásaként most a hardware és os metric adatok gyűjtésével és megjelenítésével foglalkozom.\nSystem metric adatok elérése A Prometheus egyik hivatalos metric exporter-je a Node exporter , amely pont megfelel céljaim számára.\nA node exporter futtatható docker container-ként és docker-compose service-ként az alábbi módon hoztam létre:\nnode_exporter: image: quay.io/prometheus/node-exporter:latest container_name: node_exporter command: --path.rootfs=/host pid: host ports: - \u0026#34;9100:9100\u0026#34; volumes: - /:/host:ro,rslave restart: unless-stopped amelyet a grafana-stack docker-compose.yml fájlba vettem fel, hogy egy docker network-on legyen a stack többi részével.","title":"Raspberry Pi 3 - Part 6: Server Monitoring Part 2 - System Metrics"},{"content":"A Raspberry Pi server monitorozását a Grafana Labs eszközeivel kívánom megoldani.\nA célom a rendszer és a docker container-ek monitorozása, az adatok megjelenítése grafikonokon - úgynevezett dashboard-on -, a rendszer és docker container logok egységes kezelése, riasztások küldése nem kívánt állapotok esetén.\nA Grafana Labs főbb eszközei:\nGrafana: metric adat vizualizáló, kereső és riszasztás kezelő rendszer Prometheus: metric adatokat gyűjtő és tároló rendszer Loki: log aggregáló rendszer Promtail: agent, amely logokat továbbít Loki-nak. Grafana stack létrehozása A Grafana stack elemeit docker container-ekben kívánom futtatni.\nEnnek érdekében elsőként hozzunk létre egy könytárat, ahol tárolni fogjuk a stack config adatait:\nmkdir /opt/services/grafana-stack Amely tartalmazza az alábbi könyvtárakat és fájlokat:\n. ├── docker-compose.yml ├── grafana │ └── provisioning │ └── datasources │ ├── loki_ds.yml │ └── prometheus_ds.yml ├── prometheus │ └── prometheus.yml └── promtail └── promtail-config.yml Grafana stack docker-compose.yml A grafana stack-em service-eit az alábbi docker-compose.yml fájl tartalmazza:\nversion: \u0026#39;3\u0026#39; volumes: grafana-data: driver: local prometheus-data: driver: local loki-data: driver: local services: grafaana: image: grafana/grafana:latest container_name: grafana depends_on: - prometheus - loki ports: - \u0026#34;3000:3000\u0026#34; volumes: - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources - grafana-data:/var/lib/grafana restart: unless-stopped prometheus: image: prom/prometheus:latest container_name: prometheus command: --web.enable-lifecycle --config.file=/etc/prometheus/prometheus.yml ports: - \u0026#34;9090:9090\u0026#34; volumes: - ./prometheus:/etc/prometheus - prometheus-data:/prometheus restart: unless-stopped loki: image: grafana/loki:latest container_name: loki depends_on: - promtail command: -config.file=/etc/loki/local-config.yaml ports: - \u0026#34;3100:3100\u0026#34; volumes: - loki-data:/loki restart: unless-stopped promtail: image: grafana/promtail:latest container_name: promtail command: -config.file=/etc/promtail/config.yml volumes: - ./promtail/promtail-config.yaml:/etc/promtail/config.yml restart: unless-stopped Grafana service A Grafana service-nek a ./grafana/provisioning/datasources könyvtárat, mint volume-ot állítottam be, ezen könyvtárban tárolom a használni kívánt datasource-okat (loki és prometheus).\nA Loki datasource config fájlja a grafana/provisioning/datasources/loki_ds.yml:\napiVersion: 1 datasources: - name: Loki acess: proxy type: loki url: http://loki:3100 A Prometheus datasource config fájlja a grafana/provisioning/datasources/prometheus_ds.yml:\napiVersion: 1 datasources: - name: Prometheus access: proxy type: prometheus url: http://prometheus:9090 Prometheus service A Prometheus service-nek a ./prometheus könyvtárat, mint volume-ot állítom be, hogy az itt található prometheus.yml fájlt a docker container-en kívül el lehessen érni. Ez a fájl a Prometheus config fájlja, itt lehet felvenni azon forrásokat, amelyek metric adatokat képesek szolgáltatni.\nA Prometheus indítását a command részben megadott alábbi két paraméterrel finomhangoltam:\n--config.file=/etc/prometheus/prometheus.yml: Itt adjuk át Promethusnak, hogy hol találja a használni kívánt config fájlt. --web.enable-lifecycle: Lehetőséget biztosít a prometheus.yml config fájl újrabeolvasására a /-/reload végponton, vagy az alábbi paranccsal: curl -X POST http://localhost:9090/-/reload A Prometheus dokumentációjában azt olvashatjuk, hogy alapbeállításként 15 napig őrzi meg az adatokat, a régebbieket törli.\nA prometheus/prometheus.yml:\nglobal: scrape_interval: 5s scrape_configs: # ide sorolhatjuk fel a metric adatokat szolgáltató forrásokat Loki service Loki a log aggregáló, de a logok gyűjtését a Promtail cliens vagy agent végzi.\nPromtail service A Promtail service-nek a ./promtail/promtail-config.yaml fájlt adtam át, ez a fájl a promtail config fájlja, amelyben megadhatjuk a log forrásokat:\nserver: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: # ide sorolhatjuk fel a log forrásokat Grafana stack indítása A Grafana stack a docker-compose up -d paranccsal indítható. Miután minden elindult a grafana böngészőből a \u0026lt;server url\u0026gt;:3000 címen elérhető. A alapértelmezett felhasználó admin és jelszava admin, amit belépést követően meg kell változtatnunk.\nBelépést követően a Configuration -\u0026gt; Data sources menüpontban látnunk kell a configfájlokban megadott két datasource-unkat: Loki és Prometheus, melyeket tesztelhetünk is, hogy Grafana számára elérhetőek-e.\nA Prometheus webes felülete a \u0026lt;server url\u0026gt;:9090 címen érhető el, a Status -\u0026gt; Targets menüben jelenleg nem találunk még semmit, de amint a prometheus.yml fájlban felveszünk metric adat forrásokat, azokat itt megtekinthetjük.\nÖsszegzés Jelen bejegyzésben egy alap Grafana stack létrehozását vittem végig, a következő bejegyzésekben fogok rátérni az adatok gyűjtésére és megjelenítésére.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-05-server-monitoring-part-01-grafana-stack/","summary":"A Raspberry Pi server monitorozását a Grafana Labs eszközeivel kívánom megoldani.\nA célom a rendszer és a docker container-ek monitorozása, az adatok megjelenítése grafikonokon - úgynevezett dashboard-on -, a rendszer és docker container logok egységes kezelése, riasztások küldése nem kívánt állapotok esetén.\nA Grafana Labs főbb eszközei:\nGrafana: metric adat vizualizáló, kereső és riszasztás kezelő rendszer Prometheus: metric adatokat gyűjtő és tároló rendszer Loki: log aggregáló rendszer Promtail: agent, amely logokat továbbít Loki-nak.","title":"Raspberry Pi 3 - Part 5: Server Monitoring Part 1 - Grafana Stack"},{"content":"Docker container-ben futó alkalmazásokat nem tudunk update-elni, mivel a docker container-ek immutable-ek. Ahhoz, hogy az alkalmazás újabb verzióját használhassuk szükségünk van egy docker image-re, amely tartalmazza az új verziót és a container-ünket törölnünk kell, majd az új image-el újra létre kell hoznunk.\nWatchtower A watchtower segítségével automatizálhatjuk a docker container-jeink update-elési folyamatát.\nA watchtower monitorozza a futó Docker container-jeinket, hogy a container-ek alapjáúl szolgáló image-k változtak-e. Ha úgy érzékeli, hogy volt változás, akkor automatikusan újraindítja az érintett containereket az új image-ekre épülve, megtartva az eredeti beállításokat (portok, stb.).\nA watchtower önmaga is docker container-ben fut, az alábbi paranccsal debug módban egyszeri futtatással indíthatjuk el, tesztelve a működését:\ndocker run --name watchtower \\ -v /var/run/docker.sock:/var/run/docker.sock \\ containrrr/watchtower \\ --run-once --debug (futás után a docker container-t töröljük, nincs már szükségünk rá, mert a későbbiekben úgy hozzuk létre, hogy időzítve fusson.)\nA futás eredményeként kapott logokból láthatjuk milyen lépéseket hajt végre és ha épp aktuálisan szükséges volt, akkor update-elt is néhány container-t.\nA watchtower csak azon container-eket veszi figyelembe, amelyek alapjáúl szolgáló image-ek tegjeként a latest-et adtuk meg.\nVan lehetőségünk explicit exclude-olni container-eket, hogy egyáltalán ne is figyelje a watchtower, ennek mikéntjéről a container selection részben számolnak be.\nAuto update beállítása A watchtower container-t nem paranccsal szeretném létrehozni, hanem docker-compse-al.\nÚgy döntöttem, hogy a /opt/services könyvtárban fogom tárolni a docker-compose service fájljaimat, minden service számára külön könyvtárban.\nsudo mkdir -p /opt/services/watchtower Módosítottam a services könyvtár jogosultságát, hogy csak a tulajdonos számára legyen olvasási, írási és futtatási joga:\nsudo chmod -R 700 /opt/services Majd beállítottam a könyvtár tulajdonosának a pi user-t (ez az aktuális user-em):\nsudo chown -R pi:pi /opt/services A watchtower könyvtárban létrehoztam adocker-compose.yml fájlt, aminek a tartalma:\nversion: \u0026#34;3\u0026#34; services: watchtower: image: containrrr/watchtower container_name: watchtower command: --debug --cleanup --schedule \u0026#34;0 30 3 * * *\u0026#34; volumes: - /var/run/docker.sock:/var/run/docker.sock restart: unless-stopped A command részben adtam meg a szükséges paramétereket:\n--cleanup: régi image-ek törlése update után --schedule \u0026quot;0 30 3 * * *\u0026quot;: időzített futás beállítása minden nap 3:30-ra. Futtatva a docker-compose up -d parancsot az időzített watchtower container elindul, a docker-compose logs paranccsal megnézhetjük a watchtower container logját, amely tartalmazza a következő futás időpontját.\nEzzel beállítottam a docker container-ek automatikus update-elését.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-04-docker-container-auto-update/","summary":"Docker container-ben futó alkalmazásokat nem tudunk update-elni, mivel a docker container-ek immutable-ek. Ahhoz, hogy az alkalmazás újabb verzióját használhassuk szükségünk van egy docker image-re, amely tartalmazza az új verziót és a container-ünket törölnünk kell, majd az új image-el újra létre kell hoznunk.\nWatchtower A watchtower segítségével automatizálhatjuk a docker container-jeink update-elési folyamatát.\nA watchtower monitorozza a futó Docker container-jeinket, hogy a container-ek alapjáúl szolgáló image-k változtak-e. Ha úgy érzékeli, hogy volt változás, akkor automatikusan újraindítja az érintett containereket az új image-ekre épülve, megtartva az eredeti beállításokat (portok, stb.","title":"Raspberry Pi 3 - Part 4: Docker Container Auto Update"},{"content":"Előfeltételek A docker telepítése előtt a Raspberry Pi 3 Model B+ esetében szükséges az alábbi cgroups beállításokkal kiegészíteni a /boot/cmdline.txt fájlt:\ncgroup_enable=memory cgroup_memory=1 cgroup_enable=cpuset swapaccount=1 A módosítást követően újra kell indítsuk a pi-t, hogy érvényesüljenek a kernel módosítások:\nsudo reboot Újraindulást követően a sudo cat /proc/cgroups eredményeként látnunk kell a cpuset és memory-t a listán.\nDocker telepítése Követve a hivatalos dokumentációt Raspbian esetén a convenience script segítségével telepíthetjük a Docker Engine-t.\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Ezt követően ellenőrizzük le, hogy minden rendben van-e a docker-rel:\ndocker info Ha minden rendben van, akkor sok információt kapunk a docker-ről és nem lesz hiba vagy warning.\nDocker kezelése non-root felhasználóval A docker parancs csak root jogosúltsággal használható, azaz csak a root userrel, vagy a sudo paranccsal.\nNon-root user esetén, hogy a sudo használatát mellőzhessem ismét a hivatalos leírást követtem:\nEgy docker group létrehozása: sudo groupadd docker A user felvétele a docker group-ba: sudo usermod -aG docker $USER Log out és log in, hogy a group membership újra kiértékelődjön. A group paranccsal leellenőrizhetjük, hogy a user megkapta-e a docker group-ot: groups | grep docker Hello World container futtatása A Hello World container futtatásával tesztelhetjük, hogy a Docker telepítés sikeres volt-e:\ndocker run hello-world Docker log beállítások A post-installation steps közül a logging driver beállítását találtam még hasznosnak, mert alap esetben a docker container-ek log méretének nincs határ szabva.\nAz alapértelmezett logging driver a json-file. Ha bekapcsoljuk a log rotation-t, akkor a végtelen log problémát elkerülhetjük.\nAz /etc/docker/ könyvtárba hozzunk létre egy daemon.json fájlt a következő tartalommal:\n{ \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34; } } Így containerenként maximum 30 MB logunk lesz.\nAhhoz, hogy érvényesüljenek a beállítások újra kell indítani a Docker-t:\nsudo systemctl restart docker Továbbá csak újonnan létrehozott container-ek esetében érényesül az új logging beállítás.\nEz nem egy szükséges beállítás, mert minden container esetében megadható a használni kívánt logging-driver, illetve ott azt testre is szabhatjuk.\nDocker Compose telepítése Docker Compose telepítése pip segítségével, ha nincs python3 és pip3 telepítve, akkor azokat előbb fel kell tennünk:\nsudo apt-get install -y libffi-dev libssl-dev sudo apt-get install -y python3-dev sudo apt-get install -y python3 python3-pip Majd telepíthetjük a docker-compose-t:\nsudo pip install docker-compose ","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-03-install-docker/","summary":"Előfeltételek A docker telepítése előtt a Raspberry Pi 3 Model B+ esetében szükséges az alábbi cgroups beállításokkal kiegészíteni a /boot/cmdline.txt fájlt:\ncgroup_enable=memory cgroup_memory=1 cgroup_enable=cpuset swapaccount=1 A módosítást követően újra kell indítsuk a pi-t, hogy érvényesüljenek a kernel módosítások:\nsudo reboot Újraindulást követően a sudo cat /proc/cgroups eredményeként látnunk kell a cpuset és memory-t a listán.\nDocker telepítése Követve a hivatalos dokumentációt Raspbian esetén a convenience script segítségével telepíthetjük a Docker Engine-t.","title":"Raspberry Pi 3 - Part 3: Install docker and docker-compose"},{"content":"Debian alapú rendszerek esetén az unattended-upgrades csomag biztosít lehetőséget update-ek automatikus futtatására.\nTelepítés sudo apt-get install unattended-upgrades apt-listchanges -y Beállítások Az unattended-upgrades beállításait az /etc/apt/apt.conf.d/50unattended-upgrades fájl tartalmazza.\nLehetőségünk van beállítani az automatikus újraindítást, ha az update után az szükséges:\nUnattended-Upgrade::Automatic-Reboot \u0026#34;true\u0026#34;; Unattended-Upgrade::Automatic-Reboot-Time \u0026#34;03:00\u0026#34;; Továbbá lehetőségünk van e-mail értesítést is küldeni az alábbi két sorral:\nUnattended-Upgrade::Mail \u0026#34;user@example.com\u0026#34;; Unattended-Upgrade::MailReport \u0026#34;on-change\u0026#34;; De ez csak akkor fog működni, ha a rendszer tud e-mail-t küldeni, ehhez pedig egy olyan tool-ra van szükségünk, ami a mailx package-t támogatja.\nJelenleg úgy döntöttem e-mail küldésre nincs szükségem.\nAktiválás Az unattended-upgrades aktiválásához az alábbi parancsot futtassuk:\nsudo dpkg-reconfigure -plow unattended-upgrades Ezen parancs hatására keletkezik az /etc/apt/apt.conf.d/20auto-upgrades fájl.\nA default beállításokat meghagytam, csak kiegészítettem egy továbbival, aminek hatására 21 naponta auto clean is fut.\nAPT::Periodic::Update-Package-Lists \u0026#34;1\u0026#34;; APT::Periodic::Unattended-Upgrade \u0026#34;1\u0026#34;; APT::Periodic::AutocleanInterval \u0026#34;21\u0026#34;; További beállítási lehetőségeket az alábbi helyekről lehet kinézni:\n/etc/cron.daily/apt-compat /usr/lib/apt/apt.systemd.daily Végül pedig dry-run módban (ami csak szimulálja a futást, de nem telepít) és debug infokkal ellátva futtassuk az unattended-upgrade-t, hogy megfelel-e elvárásainknak:\nsudo unattended-upgrade --dry-run --debug A futások logját az alábbi helyen találjuk: /var/log/unattended-upgrades/unattended-upgrades.log\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-02-auto-update/","summary":"Debian alapú rendszerek esetén az unattended-upgrades csomag biztosít lehetőséget update-ek automatikus futtatására.\nTelepítés sudo apt-get install unattended-upgrades apt-listchanges -y Beállítások Az unattended-upgrades beállításait az /etc/apt/apt.conf.d/50unattended-upgrades fájl tartalmazza.\nLehetőségünk van beállítani az automatikus újraindítást, ha az update után az szükséges:\nUnattended-Upgrade::Automatic-Reboot \u0026#34;true\u0026#34;; Unattended-Upgrade::Automatic-Reboot-Time \u0026#34;03:00\u0026#34;; Továbbá lehetőségünk van e-mail értesítést is küldeni az alábbi két sorral:\nUnattended-Upgrade::Mail \u0026#34;user@example.com\u0026#34;; Unattended-Upgrade::MailReport \u0026#34;on-change\u0026#34;; De ez csak akkor fog működni, ha a rendszer tud e-mail-t küldeni, ehhez pedig egy olyan tool-ra van szükségünk, ami a mailx package-t támogatja.","title":"Raspberry Pi 3 - Part 2: Auto update"},{"content":"Az OS telepítése A célom hogy headless módon használhassam a pi-t, így nincs szükségem desktop enviromentre. Követve a hivatalos getting started leírást, az OS-t a Raspberry Pi Imager segítségével telepítem.\nMiután az SD kártyát a számítógépemhez csatlakoztattam az RP Imager-ben OS-ként nem a default beállítást választottam, hanem az other opció alatt elérhető DE nélküli változatot (ebből két verzió létezik: Lite és Full, az utóbbit választottam).\nAz Advanced Options segítségével be tudtam állítani a wifi-t, a locale beállításokat és az ssh-t.\nAz alapos előkészületeket követően, megnyomtam a WRITE gombot és némi idő elteltével be is fejeződött a telepítés.\nTelepítést követően Az SD kártyát a pi-be helyezve, majd az áram alá téve hamarosan el is volt érhető a hálózaton. Besshztam a telpítés előtti advancen options-ben beállított user és password segítsévégével.\nElső parancsommal a rendszert frissítettem:\nsudo apt update -y \u0026amp;\u0026amp; sudo apt full-upgrade -y Ennek végeztével el is kezdődhet a kaland.\n","permalink":"https://thomastrinn.github.io/blog/posts/pi3-server-part-01-setup/","summary":"Az OS telepítése A célom hogy headless módon használhassam a pi-t, így nincs szükségem desktop enviromentre. Követve a hivatalos getting started leírást, az OS-t a Raspberry Pi Imager segítségével telepítem.\nMiután az SD kártyát a számítógépemhez csatlakoztattam az RP Imager-ben OS-ként nem a default beállítást választottam, hanem az other opció alatt elérhető DE nélküli változatot (ebből két verzió létezik: Lite és Full, az utóbbit választottam).\nAz Advanced Options segítségével be tudtam állítani a wifi-t, a locale beállításokat és az ssh-t.","title":"Raspberry Pi 3 - Part 1: Setup"},{"content":"A Docker Compose egy olyan eszköz, aminek segítségével definiálni és elindítani tudunk több docker container-t.\nDocker Compose fájlok A docker-compose.yml fájl tartalmazza azon service-eket amelyeket docker container-ként akarunk futtatni.\nA .env fájl nem kötelező, de ha használjuk, akkor környezeti változókat tartalmazhat, amelyek a docker-compose.yml fájlban behivatkozhatóak. További részletek itt találhatóak.\nRendelkezésre állnak előre definiált környezeti változók, amelyekkel finomhangolhatjuk a Docker Compose működését. Az egyik ilyen környezeti változó a COMPOSE_PROJECT_NAME, amellyel a projektünk nevét adhatjuk meg, amit a docker-compose utána felhasznál prefixként a létrehozandó container-ek, volume-ok, network-ok nevéhez.\nAz elérhető környezeti változók megtalálhatóak a Compose CLI environment variables oldalon.\nDocker Compose kezelése Definíció ellenőrzése Miután elkészítettük a docker-compose.yml fájl és esetleg a .env fájl, úgy az alábbi paranccsal leellenőrizhetjük azok helyességét:\ndocker-compose config Service-k indítása docker-compose up -d A -d kapcsolóval háttérfolyamatként indítja el a service-ket.\nFutó service-ek megtekintése docker-compose ps Futó servvice-k log-jának megtekintése docker-compose logs -f A -f kapcsoló követi a log-ot.\nFutó service-ek leállítása docker-compose stop Service-k törlése docker-compose down Ha futnak a service-ek akkor azok leállításra kerülnek, majd a container-ek törlése. Ha a -v kapcsolót is megadjuk, akkor a volume-ok is törlésre kerülnek.\n","permalink":"https://thomastrinn.github.io/blog/posts/get-started-with-docker-compose/","summary":"A Docker Compose egy olyan eszköz, aminek segítségével definiálni és elindítani tudunk több docker container-t.\nDocker Compose fájlok A docker-compose.yml fájl tartalmazza azon service-eket amelyeket docker container-ként akarunk futtatni.\nA .env fájl nem kötelező, de ha használjuk, akkor környezeti változókat tartalmazhat, amelyek a docker-compose.yml fájlban behivatkozhatóak. További részletek itt találhatóak.\nRendelkezésre állnak előre definiált környezeti változók, amelyekkel finomhangolhatjuk a Docker Compose működését. Az egyik ilyen környezeti változó a COMPOSE_PROJECT_NAME, amellyel a projektünk nevét adhatjuk meg, amit a docker-compose utána felhasznál prefixként a létrehozandó container-ek, volume-ok, network-ok nevéhez.","title":"Get Started With Docker Compose"},{"content":"Az alábbi docker-compose.yml fájl egy elasticsearch és egy elastickibana service-t indít el.\nversion: \u0026#39;3\u0026#39; services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.9.3 container_name: ebla_elasticsearch environment: - xpack.security.enabled=false - discovery.type=single-node ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 cap_add: - IPC_LOCK volumes: - elasticsearch_data:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 kibana: image: docker.elastic.co/kibana/kibana:7.9.3 container_name: ebla_kibana environment: - ELASTICSEARCH_HOSTS=http://elasticsearch:9200 ports: - 5601:5601 depends_on: - elasticsearch volumes: elasticsearch_data: driver: local Ezen beállítások development környezetre ajánlatosak, production esetén ez kerülendő. További információk itt találhatóak.\n","permalink":"https://thomastrinn.github.io/blog/posts/elasticsearch-in-docker/","summary":"Az alábbi docker-compose.yml fájl egy elasticsearch és egy elastickibana service-t indít el.\nversion: \u0026#39;3\u0026#39; services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.9.3 container_name: ebla_elasticsearch environment: - xpack.security.enabled=false - discovery.type=single-node ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 cap_add: - IPC_LOCK volumes: - elasticsearch_data:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 kibana: image: docker.elastic.co/kibana/kibana:7.9.3 container_name: ebla_kibana environment: - ELASTICSEARCH_HOSTS=http://elasticsearch:9200 ports: - 5601:5601 depends_on: - elasticsearch volumes: elasticsearch_data: driver: local Ezen beállítások development környezetre ajánlatosak, production esetén ez kerülendő.","title":"Elasticsearch in Docker"},{"content":"A SonarQube egy kód minőség elemző eszköz.\nSonarQube futtatása docker-compose service-ként docker-compose.yml:\nversion: \u0026#34;3\u0026#34; services: sonarqube: image: sonarqube:8.9-community container_name: sonarqube depends_on: - db ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 environment: SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar SONAR_JDBC_USERNAME: sonar SONAR_JDBC_PASSWORD: sonar volumes: - sonarqube_data:/opt/sonarqube/data - sonarqube_extensions:/opt/sonarqube/extensions - sonarqube_logs:/opt/sonarqube/logs ports: - \u0026#34;9010:9000\u0026#34; db: image: postgres:12 container_name: sonarqube-postgres environment: POSTGRES_USER: sonar POSTGRES_PASSWORD: sonar ports: - 14432:5432 volumes: - postgresql:/var/lib/postgresql - postgresql_data:/var/lib/postgresql/data volumes: sonarqube_data: sonarqube_extensions: sonarqube_logs: postgresql: postgresql_data: SonarQube indítása: docker-compose up -d\nMivel a SonarQube embedded Elasticserch-t használ szükség lehet a host gépen a következő beállításra:\nsysctl -w vm.max_map_count=524288 Ha még így is gond volna, akkor a SonarQube Docker Hub oldalból induljál ki.\nSikeres indítás követően az alkalmazás elérhető a http://localhost:9010/ url-en.\nBelépni admin/admin segítségével lehet.\nSonar Scanner futtatása docker container-ből Ha sonar-scanner-t kell futtatnunk a projektünk forráskódján, akkor azt is megtehetjük docker container-ből, az alábbi módon:\ndocker run --rm \\ --net host \\ -v \u0026#34;$(pwd):/usr/src\u0026#34; \\ sonarsource/sonar-scanner-cli \\ -Dsonar.projectKey=\u0026lt;your-projectKey\u0026gt; \\ -Dsonar.host.url=\u0026lt;your-sonar-host\u0026gt; \\ -Dsonar.login=\u0026lt;your-login\u0026gt; A -v \u0026quot;$(pwd):/usr/src\u0026quot; azt a mapping-et mutatja, hogy ha a forráskódunk könyvtárában adjuk ki ezt a parancsot, akkor az a container /usr/src könyvtárára mutat, amelyben a sonar-scanner keresi a forráskódot.\nA -Dsonar.* paraméterek értékét meg a sonarqube oldal a project létrehozásakor megad nekünk, s azokat kell ide behelyettesítenünk.\n","permalink":"https://thomastrinn.github.io/blog/posts/sonarqube-in-docker/","summary":"A SonarQube egy kód minőség elemző eszköz.\nSonarQube futtatása docker-compose service-ként docker-compose.yml:\nversion: \u0026#34;3\u0026#34; services: sonarqube: image: sonarqube:8.9-community container_name: sonarqube depends_on: - db ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 environment: SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar SONAR_JDBC_USERNAME: sonar SONAR_JDBC_PASSWORD: sonar volumes: - sonarqube_data:/opt/sonarqube/data - sonarqube_extensions:/opt/sonarqube/extensions - sonarqube_logs:/opt/sonarqube/logs ports: - \u0026#34;9010:9000\u0026#34; db: image: postgres:12 container_name: sonarqube-postgres environment: POSTGRES_USER: sonar POSTGRES_PASSWORD: sonar ports: - 14432:5432 volumes: - postgresql:/var/lib/postgresql - postgresql_data:/var/lib/postgresql/data volumes: sonarqube_data: sonarqube_extensions: sonarqube_logs: postgresql: postgresql_data: SonarQube indítása: docker-compose up -d","title":"SonarQube in Docker"},{"content":"PostgreSQL server könnyen indítható docker container segítségével. A container létrehozását docker-compose segítségével végzem.\nPostgreSQL docker-compose service Ahhoz, hogy docker-compose segítségével hozzunk létre a postgresql docker containert két fájlra lesz szükségünk. Az egyik a .env fájl a másik a docker-compose.yml.\nA .env fájl nem kötelező, de praktikus, itt környezeti változókat definiálhatunk, amelyeket a docker-compose.yml fájlban fel tudunk használni.\nA docker-compose.yml fájlban definiáljuk a service-ket a volumes-eket. Jelen esetben csak egy service-t definiálunk, a postgresql-t.\nA .env fájl tartalma:\nCOMPOSE_PROJECT_NAME=postgres POSTGRES_PASSWORD=postgres POSTGRES_USER=postgres POSTGRES_DB=postgres A COMPOSE_PROJECT_NAME környezeti változóval a project nevét adhatjuk meg, részleteket itt olvashatunk.\nA többi környezeti változót pedig a postgresql service fogja felhasználni.\nA docker-compose.yml fájl tartalma:\nversion: \u0026#39;3.8\u0026#39; services: postgres: image: postgres:12 container_name: postgres environment: - POSTGRES_USER=${POSTGRES_USER} - POSTGRES_PASSWORD=${POSTGRES_PASSWORD} - POSTGRES_DB=${POSTGRES_DB} command: postgres -c max_connections=150 volumes: - postgres:/var/lib/postgresql/data ports: - 5432:5432 volumes: postgres: driver: local A .env-ben definiált környezeti változókat itt az enviroment: szakaszban használjuk fel, adjuk át a container-nek induláskor.\nEgy postgres nevű volume-ot definiálunk, ami a container-ben levő /var/lib/postgresql/data könyvtárra mutat. Így azét érjük el, hogy a container törlése után a postgres adatok megmaradnak és a container újra létrehozását követően meglesznek az adatbázis adatok.\nA container-ben futó postgre adatbázis beállítását a command: résznél végezhetjük el, a postgres -c \u0026lt;param1=value1\u0026gt; -c \u0026lt;param2=value2\u0026gt; parancsot megadva. A fenti esetben a max_connections értékét adjuk meg (alap értéke 100).\nKiterjedtebb leírást a postgres dockerhub oldalon találunk.\nPostgreSQL tuning Egy adott környezetre a beállítások testreszabása érdekében a következő két forrást tudom javasolni:\nTuning your PostgreSQL Server: részletes leírása az egyes beállításoknak PGTune: egyszerű eszköz amivel könnyedén előállíthatjuk a config értékeket. ","permalink":"https://thomastrinn.github.io/blog/posts/postgresql-in-docker/","summary":"PostgreSQL server könnyen indítható docker container segítségével. A container létrehozását docker-compose segítségével végzem.\nPostgreSQL docker-compose service Ahhoz, hogy docker-compose segítségével hozzunk létre a postgresql docker containert két fájlra lesz szükségünk. Az egyik a .env fájl a másik a docker-compose.yml.\nA .env fájl nem kötelező, de praktikus, itt környezeti változókat definiálhatunk, amelyeket a docker-compose.yml fájlban fel tudunk használni.\nA docker-compose.yml fájlban definiáljuk a service-ket a volumes-eket. Jelen esetben csak egy service-t definiálunk, a postgresql-t.","title":"PostgreSQL in Docker"},{"content":"Journal mérete A journal a /var/log/journal/ könyvtárba hozza létre a log fájlokat, amik idővel sok helyet foglalhatnak. Ennek határt szabhatunk.\nA /etc/systemd/journald.conf fájlban a SystemMaxUse változónak értéket adhatunk:\nSystemMaxUse=50M A config fájl módosítását követően a journal service-t újra kell indítanunk, hogy a változtatások érvényesüljenek:\nsystemctl restart systemd-journald Hibák megtekintése Az alábbi paranccsal a log-ban levő hibákat tekinthetjük meg:\njournalctl -r -p 3 További részletek: arch wiki journal\n","permalink":"https://thomastrinn.github.io/blog/posts/linux-systemd-journal/","summary":"Journal mérete A journal a /var/log/journal/ könyvtárba hozza létre a log fájlokat, amik idővel sok helyet foglalhatnak. Ennek határt szabhatunk.\nA /etc/systemd/journald.conf fájlban a SystemMaxUse változónak értéket adhatunk:\nSystemMaxUse=50M A config fájl módosítását követően a journal service-t újra kell indítanunk, hogy a változtatások érvényesüljenek:\nsystemctl restart systemd-journald Hibák megtekintése Az alábbi paranccsal a log-ban levő hibákat tekinthetjük meg:\njournalctl -r -p 3 További részletek: arch wiki journal","title":"Linux Systemd Journal"}]